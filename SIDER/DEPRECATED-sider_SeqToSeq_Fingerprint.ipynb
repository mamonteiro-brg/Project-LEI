{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "A SeqToSeq model performs sequence to sequence translation. For example, they are often used to translate text from one language to another. It consists of two parts called the “encoder” and “decoder”. The encoder is a stack of recurrent layers. The input sequence is fed into it, one token at a time, and it generates a fixed length vector called the “embedding vector”. The decoder is another stack of recurrent layers that performs the inverse operation: it takes the embedding vector as input, and generates the output sequence. By training it on appropriately chosen input/output pairs, you can create a model that performs many sorts of transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mamonteiro/anaconda3/envs/lei/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n"
     ]
    }
   ],
   "source": [
    "import deepchem as dc\n",
    "tasks, datasets, transformers = dc.molnet.load_sider()\n",
    "train_dataset, valid_dataset, test_dataset = datasets\n",
    "train_smiles = train_dataset.ids\n",
    "valid_smiles = valid_dataset.ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1141"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_smiles.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['C(CNCCNCCNCCN)N',\n",
       "       'CC(C)(C)C1=CC(=C(C=C1NC(=O)C2=CNC3=CC=CC=C3C2=O)O)C(C)(C)C',\n",
       "       'CC[C@]12CC(=C)[C@H]3[C@H]([C@@H]1CC[C@]2(C#C)O)CCC4=CCCC[C@H]34',\n",
       "       ..., 'CC12CC(C3(C(C1CC(C2(C(=O)CO)O)O)CCC4=CC(=O)C=CC43C)F)O',\n",
       "       'CC1=CC(=CC(=C1OC2=NC(=NC(=C2Br)N)NC3=CC=C(C=C3)C#N)C)C#N',\n",
       "       'CC1=NN=C2N1C3=C(C=C(C=C3)Cl)C(=NC2)C4=CC=CC=C4Cl'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['C(CNCCNCCNCCN)N',\n",
       "       'CC(C)(C)C1=CC(=C(C=C1NC(=O)C2=CNC3=CC=CC=C3C2=O)O)C(C)(C)C',\n",
       "       'CC[C@]12CC(=C)[C@H]3[C@H]([C@@H]1CC[C@]2(C#C)O)CCC4=CCCC[C@H]34',\n",
       "       ..., 'CC12CC(C3(C(C1CC(C2(C(=O)CO)O)O)CCC4=CC(=O)C=CC43C)F)O',\n",
       "       'CC1=CC(=CC(=C1OC2=NC(=NC(=C2Br)N)NC3=CC=C(C=C3)C#N)C)C#N',\n",
       "       'CC1=NN=C2N1C3=C(C=C(C=C3)Cl)C(=NC2)C4=CC=CC=C4Cl'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = set()\n",
    "for s in train_smiles:\n",
    "  tokens = tokens.union(set(c for c in s))\n",
    "tokens = sorted(list(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.get_task_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchem.models.tensorgraph.optimizers import Adam, ExponentialDecay\n",
    "max_length = max(len(s) for s in train_smiles)\n",
    "model = dc.models.SeqToSeq(tokens,\n",
    "                           tokens,\n",
    "                           max_length,\n",
    "                           encoder_layers=2,\n",
    "                           decoder_layers=2,\n",
    "                           embedding_dimension=256,\n",
    "                           model_dir='fingerprint')\n",
    "batches_per_epoch = len(train_smiles)/model.batch_size\n",
    "model.set_optimizer(Adam(learning_rate=ExponentialDecay(0.004, 0.9, batches_per_epoch)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequences(epochs):\n",
    "  for i in range(epochs):\n",
    "    for s in train_smiles:\n",
    "      yield (s, s)\n",
    "\n",
    "model.fit_sequences(generate_sequences(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict_from_sequences(valid_smiles[:500])\n",
    "count = 0\n",
    "for s,p in zip(valid_smiles[:500], predicted):\n",
    "  if ''.join(p) == s:\n",
    "    count += 1\n",
    "print('reproduced', count, 'of 500 validation SMILES strings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.92059219, 0.43273092, 1.        , ..., 0.44433198, 0.09432515,\n",
       "        1.        ],\n",
       "       [1.        , 0.43273092, 1.        , ..., 1.        , 0.09432515,\n",
       "        1.        ],\n",
       "       [1.        , 0.43273092, 1.        , ..., 1.        , 0.09432515,\n",
       "        1.        ],\n",
       "       ...,\n",
       "       [0.92059219, 0.43273092, 1.        , ..., 0.44433198, 0.09432515,\n",
       "        0.50845666],\n",
       "       [0.92059219, 0.43273092, 1.        , ..., 0.44433198, 0.09432515,\n",
       "        0.50845666],\n",
       "       [0.92059219, 0.43273092, 1.        , ..., 0.44433198, 0.09432515,\n",
       "        0.50845666]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings = model.predict_embeddings(train_smiles)\n",
    "train_embeddings_dataset = dc.data.NumpyDataset(train_embeddings,\n",
    "                                                train_dataset.y,\n",
    "                                                train_dataset.w,\n",
    "                                                train_dataset.ids)\n",
    "\n",
    "valid_embeddings = model.predict_embeddings(valid_smiles)\n",
    "valid_embeddings_dataset = dc.data.NumpyDataset(valid_embeddings,\n",
    "                                                valid_dataset.y,\n",
    "                                                valid_dataset.w,\n",
    "                                                valid_dataset.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = dc.models.MultiTaskClassifier(n_tasks=len(tasks),\n",
    "                                                      n_features=256,\n",
    "                                                      layer_sizes=[512])\n",
    "classifier.fit(train_embeddings_dataset, nb_epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "metric = dc.metrics.Metric(dc.metrics.roc_auc_score, np.mean, mode=\"classification\")\n",
    "train_score = classifier.evaluate(train_embeddings_dataset, [metric], transformers)\n",
    "valid_score = classifier.evaluate(valid_embeddings_dataset, [metric], transformers)\n",
    "print('Training set ROC AUC:', train_score)\n",
    "print('Validation set ROC AUC:', valid_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
