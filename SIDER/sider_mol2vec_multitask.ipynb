{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mol2vec.features import mol2alt_sentence, MolSentence, DfVec, sentences2vec\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import word2vec\n",
    "from rdkit import Chem\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.callbacks import ModelCheckpoint, Callback\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    " df = pd.read_csv('/home/mamonteiro/source-code/Project-LEI/SIDER/sider.csv')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                      C(CNCCNCCNCCN)N\n",
       "1    CC(C)(C)C1=CC(=C(C=C1NC(=O)C2=CNC3=CC=CC=C3C2=...\n",
       "2    CC[C@]12CC(=C)[C@H]3[C@H]([C@@H]1CC[C@]2(C#C)O...\n",
       "3      CCC12CC(=C)C3C(C1CC[C@]2(C#C)O)CCC4=CC(=O)CCC34\n",
       "4               C1C(C2=CC=CC=C2N(C3=CC=CC=C31)C(=O)N)O\n",
       "Name: smiles, dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['smiles'][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get SMILES from file\n",
    "def getSMILES(filepath):\n",
    "    df = pd.read_csv(filepath)    \n",
    "    #smiles = list(df['SMILES or PubChem ID'].dropna())\n",
    "    smiles_df = df[['smiles']].dropna()\n",
    "    # some cases have 2 SMILES per compound, which I think are isomers (by looking at the molecular drawings)\n",
    "    # keeping only the first SMILE string:\n",
    "    #smiles = [x.split(';')[0] for x in smiles]\n",
    "    return smiles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate molecules from SMILES strings\n",
    "def generateEmbeddings(smiles_df, trained_model):\n",
    "    smiles = list(smiles_df['smiles'])\n",
    "    smiles = [x.split(';')[0] for x in smiles]\n",
    "    # SMILES to Mol\n",
    "    molecules = [Chem.MolFromSmiles(x) for x in smiles]\n",
    "    # Load previously trained mol2vec model\n",
    "    model = Word2Vec.load(trained_model)\n",
    "    # Convert molecules to sentences and then to embeddings\n",
    "    sentences = [mol2alt_sentence(x, 1) for x in molecules]\n",
    "    vectors = [DfVec(x) for x in sentences2vec(sentences, model, unseen='UNK')]\n",
    "    vec_df = pd.DataFrame(data=np.array([x.vec for x in vectors]))\n",
    "    vec_df.columns = ['mol2vec_' + str(x+1) for x in vec_df.columns.values]\n",
    "    vec_df.index = smiles_df.index.values\n",
    "    return pd.concat([smiles_df, vec_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDataset(original_data, embeddings_df, output):\n",
    "    original_df = pd.read_csv(original_data)\n",
    "    df = original_df.merge(embeddings_df, how='outer', on=\"smiles\")\n",
    "    df.to_csv(output, index=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simles_from_side =getSMILES('/home/mamonteiro/source-code/Project-LEI/SIDER/sider.csv')\n",
    "type(simles_from_side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current directory is : /home/mamonteiro/source-code/Project-LEI/SIDER\n",
      "Directory name is : SIDER\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    " \n",
    "dirpath = os.getcwd()\n",
    "print(\"current directory is : \" + dirpath)\n",
    "foldername = os.path.basename(dirpath)\n",
    "print(\"Directory name is : \" + foldername)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:smart_open.smart_open_lib:this function is deprecated, use smart_open.open instead\n",
      "WARNING:smart_open.smart_open_lib:this function is deprecated, use smart_open.open instead\n"
     ]
    }
   ],
   "source": [
    "emb_df = generateEmbeddings(simles_from_side, '/home/mamonteiro/source-code/Project-LEI/mol2vec/examples/models/model_300dim.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>mol2vec_1</th>\n",
       "      <th>mol2vec_2</th>\n",
       "      <th>mol2vec_3</th>\n",
       "      <th>mol2vec_4</th>\n",
       "      <th>mol2vec_5</th>\n",
       "      <th>mol2vec_6</th>\n",
       "      <th>mol2vec_7</th>\n",
       "      <th>mol2vec_8</th>\n",
       "      <th>mol2vec_9</th>\n",
       "      <th>...</th>\n",
       "      <th>mol2vec_291</th>\n",
       "      <th>mol2vec_292</th>\n",
       "      <th>mol2vec_293</th>\n",
       "      <th>mol2vec_294</th>\n",
       "      <th>mol2vec_295</th>\n",
       "      <th>mol2vec_296</th>\n",
       "      <th>mol2vec_297</th>\n",
       "      <th>mol2vec_298</th>\n",
       "      <th>mol2vec_299</th>\n",
       "      <th>mol2vec_300</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C(CNCCNCCNCCN)N</td>\n",
       "      <td>-0.990727</td>\n",
       "      <td>-1.723967</td>\n",
       "      <td>1.596080</td>\n",
       "      <td>0.336589</td>\n",
       "      <td>5.995870</td>\n",
       "      <td>1.602312</td>\n",
       "      <td>-7.893780</td>\n",
       "      <td>-0.770941</td>\n",
       "      <td>2.798226</td>\n",
       "      <td>...</td>\n",
       "      <td>1.845091</td>\n",
       "      <td>4.080578</td>\n",
       "      <td>5.290233</td>\n",
       "      <td>2.681949</td>\n",
       "      <td>-6.017433</td>\n",
       "      <td>0.732134</td>\n",
       "      <td>-0.161610</td>\n",
       "      <td>-7.355957</td>\n",
       "      <td>-6.514126</td>\n",
       "      <td>-4.416229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CC(C)(C)C1=CC(=C(C=C1NC(=O)C2=CNC3=CC=CC=C3C2=...</td>\n",
       "      <td>0.596306</td>\n",
       "      <td>0.060384</td>\n",
       "      <td>-4.686247</td>\n",
       "      <td>4.381831</td>\n",
       "      <td>2.139633</td>\n",
       "      <td>-0.343262</td>\n",
       "      <td>-13.849467</td>\n",
       "      <td>0.780369</td>\n",
       "      <td>9.671047</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.760953</td>\n",
       "      <td>9.614191</td>\n",
       "      <td>13.119958</td>\n",
       "      <td>-0.408570</td>\n",
       "      <td>-7.577562</td>\n",
       "      <td>-4.803534</td>\n",
       "      <td>-4.880173</td>\n",
       "      <td>-7.033062</td>\n",
       "      <td>-15.572207</td>\n",
       "      <td>-4.429869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CC[C@]12CC(=C)[C@H]3[C@H]([C@@H]1CC[C@]2(C#C)O...</td>\n",
       "      <td>1.854099</td>\n",
       "      <td>-4.819261</td>\n",
       "      <td>0.775984</td>\n",
       "      <td>3.286393</td>\n",
       "      <td>1.519570</td>\n",
       "      <td>-6.281527</td>\n",
       "      <td>-10.383826</td>\n",
       "      <td>5.000489</td>\n",
       "      <td>-0.742257</td>\n",
       "      <td>...</td>\n",
       "      <td>0.873612</td>\n",
       "      <td>15.422496</td>\n",
       "      <td>-1.348580</td>\n",
       "      <td>-1.571692</td>\n",
       "      <td>-17.224537</td>\n",
       "      <td>-6.963869</td>\n",
       "      <td>-12.192692</td>\n",
       "      <td>-3.912728</td>\n",
       "      <td>-8.017673</td>\n",
       "      <td>1.265650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CCC12CC(=C)C3C(C1CC[C@]2(C#C)O)CCC4=CC(=O)CCC34</td>\n",
       "      <td>2.140168</td>\n",
       "      <td>-5.169839</td>\n",
       "      <td>-0.039611</td>\n",
       "      <td>3.000724</td>\n",
       "      <td>0.787211</td>\n",
       "      <td>-6.880911</td>\n",
       "      <td>-10.384640</td>\n",
       "      <td>5.767844</td>\n",
       "      <td>-0.478307</td>\n",
       "      <td>...</td>\n",
       "      <td>0.627151</td>\n",
       "      <td>15.921182</td>\n",
       "      <td>-1.159732</td>\n",
       "      <td>-1.260444</td>\n",
       "      <td>-17.655016</td>\n",
       "      <td>-7.629984</td>\n",
       "      <td>-13.157971</td>\n",
       "      <td>-3.574889</td>\n",
       "      <td>-8.912206</td>\n",
       "      <td>1.509292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C1C(C2=CC=CC=C2N(C3=CC=CC=C31)C(=O)N)O</td>\n",
       "      <td>-0.227203</td>\n",
       "      <td>-2.800524</td>\n",
       "      <td>-1.099674</td>\n",
       "      <td>4.259158</td>\n",
       "      <td>-1.261016</td>\n",
       "      <td>-2.570959</td>\n",
       "      <td>-8.051775</td>\n",
       "      <td>1.025365</td>\n",
       "      <td>6.135835</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.972749</td>\n",
       "      <td>9.117423</td>\n",
       "      <td>7.184863</td>\n",
       "      <td>-0.012285</td>\n",
       "      <td>-7.312431</td>\n",
       "      <td>-1.483516</td>\n",
       "      <td>-4.117091</td>\n",
       "      <td>-2.378627</td>\n",
       "      <td>-9.008883</td>\n",
       "      <td>-0.717169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 301 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              smiles  mol2vec_1  mol2vec_2  \\\n",
       "0                                    C(CNCCNCCNCCN)N  -0.990727  -1.723967   \n",
       "1  CC(C)(C)C1=CC(=C(C=C1NC(=O)C2=CNC3=CC=CC=C3C2=...   0.596306   0.060384   \n",
       "2  CC[C@]12CC(=C)[C@H]3[C@H]([C@@H]1CC[C@]2(C#C)O...   1.854099  -4.819261   \n",
       "3    CCC12CC(=C)C3C(C1CC[C@]2(C#C)O)CCC4=CC(=O)CCC34   2.140168  -5.169839   \n",
       "4             C1C(C2=CC=CC=C2N(C3=CC=CC=C31)C(=O)N)O  -0.227203  -2.800524   \n",
       "\n",
       "   mol2vec_3  mol2vec_4  mol2vec_5  mol2vec_6  mol2vec_7  mol2vec_8  \\\n",
       "0   1.596080   0.336589   5.995870   1.602312  -7.893780  -0.770941   \n",
       "1  -4.686247   4.381831   2.139633  -0.343262 -13.849467   0.780369   \n",
       "2   0.775984   3.286393   1.519570  -6.281527 -10.383826   5.000489   \n",
       "3  -0.039611   3.000724   0.787211  -6.880911 -10.384640   5.767844   \n",
       "4  -1.099674   4.259158  -1.261016  -2.570959  -8.051775   1.025365   \n",
       "\n",
       "   mol2vec_9  ...  mol2vec_291  mol2vec_292  mol2vec_293  mol2vec_294  \\\n",
       "0   2.798226  ...     1.845091     4.080578     5.290233     2.681949   \n",
       "1   9.671047  ...    -0.760953     9.614191    13.119958    -0.408570   \n",
       "2  -0.742257  ...     0.873612    15.422496    -1.348580    -1.571692   \n",
       "3  -0.478307  ...     0.627151    15.921182    -1.159732    -1.260444   \n",
       "4   6.135835  ...    -0.972749     9.117423     7.184863    -0.012285   \n",
       "\n",
       "   mol2vec_295  mol2vec_296  mol2vec_297  mol2vec_298  mol2vec_299  \\\n",
       "0    -6.017433     0.732134    -0.161610    -7.355957    -6.514126   \n",
       "1    -7.577562    -4.803534    -4.880173    -7.033062   -15.572207   \n",
       "2   -17.224537    -6.963869   -12.192692    -3.912728    -8.017673   \n",
       "3   -17.655016    -7.629984   -13.157971    -3.574889    -8.912206   \n",
       "4    -7.312431    -1.483516    -4.117091    -2.378627    -9.008883   \n",
       "\n",
       "   mol2vec_300  \n",
       "0    -4.416229  \n",
       "1    -4.429869  \n",
       "2     1.265650  \n",
       "3     1.509292  \n",
       "4    -0.717169  \n",
       "\n",
       "[5 rows x 301 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C(CNCCNCCNCCN)N'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_df.iloc[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "createDataset('/home/mamonteiro/source-code/Project-LEI/SIDER/sider.csv', emb_df, '/home/mamonteiro/source-code/Project-LEI/SIDER/sider_embeddings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    " df = pd.read_csv('/home/mamonteiro/source-code/Project-LEI/SIDER/sider_embeddings.csv')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>Hepatobiliary disorders</th>\n",
       "      <th>Metabolism and nutrition disorders</th>\n",
       "      <th>Product issues</th>\n",
       "      <th>Eye disorders</th>\n",
       "      <th>Investigations</th>\n",
       "      <th>Musculoskeletal and connective tissue disorders</th>\n",
       "      <th>Gastrointestinal disorders</th>\n",
       "      <th>Social circumstances</th>\n",
       "      <th>Immune system disorders</th>\n",
       "      <th>...</th>\n",
       "      <th>mol2vec_291</th>\n",
       "      <th>mol2vec_292</th>\n",
       "      <th>mol2vec_293</th>\n",
       "      <th>mol2vec_294</th>\n",
       "      <th>mol2vec_295</th>\n",
       "      <th>mol2vec_296</th>\n",
       "      <th>mol2vec_297</th>\n",
       "      <th>mol2vec_298</th>\n",
       "      <th>mol2vec_299</th>\n",
       "      <th>mol2vec_300</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C(CNCCNCCNCCN)N</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.845092</td>\n",
       "      <td>4.080578</td>\n",
       "      <td>5.290233</td>\n",
       "      <td>2.681949</td>\n",
       "      <td>-6.017433</td>\n",
       "      <td>0.732134</td>\n",
       "      <td>-0.161610</td>\n",
       "      <td>-7.355957</td>\n",
       "      <td>-6.514126</td>\n",
       "      <td>-4.416229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CC(C)(C)C1=CC(=C(C=C1NC(=O)C2=CNC3=CC=CC=C3C2=...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.760953</td>\n",
       "      <td>9.614191</td>\n",
       "      <td>13.119958</td>\n",
       "      <td>-0.408570</td>\n",
       "      <td>-7.577562</td>\n",
       "      <td>-4.803534</td>\n",
       "      <td>-4.880173</td>\n",
       "      <td>-7.033062</td>\n",
       "      <td>-15.572207</td>\n",
       "      <td>-4.429869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CC[C@]12CC(=C)[C@H]3[C@H]([C@@H]1CC[C@]2(C#C)O...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.873612</td>\n",
       "      <td>15.422496</td>\n",
       "      <td>-1.348580</td>\n",
       "      <td>-1.571692</td>\n",
       "      <td>-17.224537</td>\n",
       "      <td>-6.963869</td>\n",
       "      <td>-12.192692</td>\n",
       "      <td>-3.912728</td>\n",
       "      <td>-8.017673</td>\n",
       "      <td>1.265650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CCC12CC(=C)C3C(C1CC[C@]2(C#C)O)CCC4=CC(=O)CCC34</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.627151</td>\n",
       "      <td>15.921182</td>\n",
       "      <td>-1.159732</td>\n",
       "      <td>-1.260444</td>\n",
       "      <td>-17.655016</td>\n",
       "      <td>-7.629984</td>\n",
       "      <td>-13.157971</td>\n",
       "      <td>-3.574889</td>\n",
       "      <td>-8.912206</td>\n",
       "      <td>1.509292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C1C(C2=CC=CC=C2N(C3=CC=CC=C31)C(=O)N)O</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.972749</td>\n",
       "      <td>9.117423</td>\n",
       "      <td>7.184863</td>\n",
       "      <td>-0.012285</td>\n",
       "      <td>-7.312431</td>\n",
       "      <td>-1.483516</td>\n",
       "      <td>-4.117091</td>\n",
       "      <td>-2.378627</td>\n",
       "      <td>-9.008883</td>\n",
       "      <td>-0.717169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 328 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              smiles  Hepatobiliary disorders  \\\n",
       "0                                    C(CNCCNCCNCCN)N                        1   \n",
       "1  CC(C)(C)C1=CC(=C(C=C1NC(=O)C2=CNC3=CC=CC=C3C2=...                        0   \n",
       "2  CC[C@]12CC(=C)[C@H]3[C@H]([C@@H]1CC[C@]2(C#C)O...                        0   \n",
       "3    CCC12CC(=C)C3C(C1CC[C@]2(C#C)O)CCC4=CC(=O)CCC34                        1   \n",
       "4             C1C(C2=CC=CC=C2N(C3=CC=CC=C31)C(=O)N)O                        1   \n",
       "\n",
       "   Metabolism and nutrition disorders  Product issues  Eye disorders  \\\n",
       "0                                   1               0              0   \n",
       "1                                   1               0              0   \n",
       "2                                   1               0              1   \n",
       "3                                   1               0              1   \n",
       "4                                   1               0              1   \n",
       "\n",
       "   Investigations  Musculoskeletal and connective tissue disorders  \\\n",
       "0               1                                                1   \n",
       "1               1                                                1   \n",
       "2               1                                                0   \n",
       "3               1                                                1   \n",
       "4               1                                                1   \n",
       "\n",
       "   Gastrointestinal disorders  Social circumstances  Immune system disorders  \\\n",
       "0                           1                     0                        0   \n",
       "1                           1                     0                        0   \n",
       "2                           1                     0                        1   \n",
       "3                           1                     0                        1   \n",
       "4                           1                     0                        1   \n",
       "\n",
       "   ...  mol2vec_291  mol2vec_292  mol2vec_293  mol2vec_294  mol2vec_295  \\\n",
       "0  ...     1.845092     4.080578     5.290233     2.681949    -6.017433   \n",
       "1  ...    -0.760953     9.614191    13.119958    -0.408570    -7.577562   \n",
       "2  ...     0.873612    15.422496    -1.348580    -1.571692   -17.224537   \n",
       "3  ...     0.627151    15.921182    -1.159732    -1.260444   -17.655016   \n",
       "4  ...    -0.972749     9.117423     7.184863    -0.012285    -7.312431   \n",
       "\n",
       "   mol2vec_296  mol2vec_297  mol2vec_298  mol2vec_299  mol2vec_300  \n",
       "0     0.732134    -0.161610    -7.355957    -6.514126    -4.416229  \n",
       "1    -4.803534    -4.880173    -7.033062   -15.572207    -4.429869  \n",
       "2    -6.963869   -12.192692    -3.912728    -8.017673     1.265650  \n",
       "3    -7.629984   -13.157971    -3.574889    -8.912206     1.509292  \n",
       "4    -1.483516    -4.117091    -2.378627    -9.008883    -0.717169  \n",
       "\n",
       "[5 rows x 328 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Multitask Networks On SIDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns of dataset: ['smiles' 'Hepatobiliary disorders' 'Metabolism and nutrition disorders'\n",
      " 'Product issues' 'Eye disorders' 'Investigations'\n",
      " 'Musculoskeletal and connective tissue disorders'\n",
      " 'Gastrointestinal disorders' 'Social circumstances'\n",
      " 'Immune system disorders' 'Reproductive system and breast disorders'\n",
      " 'Neoplasms benign, malignant and unspecified (incl cysts and polyps)'\n",
      " 'General disorders and administration site conditions'\n",
      " 'Endocrine disorders' 'Surgical and medical procedures'\n",
      " 'Vascular disorders' 'Blood and lymphatic system disorders'\n",
      " 'Skin and subcutaneous tissue disorders'\n",
      " 'Congenital, familial and genetic disorders'\n",
      " 'Infections and infestations'\n",
      " 'Respiratory, thoracic and mediastinal disorders' 'Psychiatric disorders'\n",
      " 'Renal and urinary disorders'\n",
      " 'Pregnancy, puerperium and perinatal conditions'\n",
      " 'Ear and labyrinth disorders' 'Cardiac disorders'\n",
      " 'Nervous system disorders'\n",
      " 'Injury, poisoning and procedural complications' 'mol2vec_1' 'mol2vec_2'\n",
      " 'mol2vec_3' 'mol2vec_4' 'mol2vec_5' 'mol2vec_6' 'mol2vec_7' 'mol2vec_8'\n",
      " 'mol2vec_9' 'mol2vec_10' 'mol2vec_11' 'mol2vec_12' 'mol2vec_13'\n",
      " 'mol2vec_14' 'mol2vec_15' 'mol2vec_16' 'mol2vec_17' 'mol2vec_18'\n",
      " 'mol2vec_19' 'mol2vec_20' 'mol2vec_21' 'mol2vec_22' 'mol2vec_23'\n",
      " 'mol2vec_24' 'mol2vec_25' 'mol2vec_26' 'mol2vec_27' 'mol2vec_28'\n",
      " 'mol2vec_29' 'mol2vec_30' 'mol2vec_31' 'mol2vec_32' 'mol2vec_33'\n",
      " 'mol2vec_34' 'mol2vec_35' 'mol2vec_36' 'mol2vec_37' 'mol2vec_38'\n",
      " 'mol2vec_39' 'mol2vec_40' 'mol2vec_41' 'mol2vec_42' 'mol2vec_43'\n",
      " 'mol2vec_44' 'mol2vec_45' 'mol2vec_46' 'mol2vec_47' 'mol2vec_48'\n",
      " 'mol2vec_49' 'mol2vec_50' 'mol2vec_51' 'mol2vec_52' 'mol2vec_53'\n",
      " 'mol2vec_54' 'mol2vec_55' 'mol2vec_56' 'mol2vec_57' 'mol2vec_58'\n",
      " 'mol2vec_59' 'mol2vec_60' 'mol2vec_61' 'mol2vec_62' 'mol2vec_63'\n",
      " 'mol2vec_64' 'mol2vec_65' 'mol2vec_66' 'mol2vec_67' 'mol2vec_68'\n",
      " 'mol2vec_69' 'mol2vec_70' 'mol2vec_71' 'mol2vec_72' 'mol2vec_73'\n",
      " 'mol2vec_74' 'mol2vec_75' 'mol2vec_76' 'mol2vec_77' 'mol2vec_78'\n",
      " 'mol2vec_79' 'mol2vec_80' 'mol2vec_81' 'mol2vec_82' 'mol2vec_83'\n",
      " 'mol2vec_84' 'mol2vec_85' 'mol2vec_86' 'mol2vec_87' 'mol2vec_88'\n",
      " 'mol2vec_89' 'mol2vec_90' 'mol2vec_91' 'mol2vec_92' 'mol2vec_93'\n",
      " 'mol2vec_94' 'mol2vec_95' 'mol2vec_96' 'mol2vec_97' 'mol2vec_98'\n",
      " 'mol2vec_99' 'mol2vec_100' 'mol2vec_101' 'mol2vec_102' 'mol2vec_103'\n",
      " 'mol2vec_104' 'mol2vec_105' 'mol2vec_106' 'mol2vec_107' 'mol2vec_108'\n",
      " 'mol2vec_109' 'mol2vec_110' 'mol2vec_111' 'mol2vec_112' 'mol2vec_113'\n",
      " 'mol2vec_114' 'mol2vec_115' 'mol2vec_116' 'mol2vec_117' 'mol2vec_118'\n",
      " 'mol2vec_119' 'mol2vec_120' 'mol2vec_121' 'mol2vec_122' 'mol2vec_123'\n",
      " 'mol2vec_124' 'mol2vec_125' 'mol2vec_126' 'mol2vec_127' 'mol2vec_128'\n",
      " 'mol2vec_129' 'mol2vec_130' 'mol2vec_131' 'mol2vec_132' 'mol2vec_133'\n",
      " 'mol2vec_134' 'mol2vec_135' 'mol2vec_136' 'mol2vec_137' 'mol2vec_138'\n",
      " 'mol2vec_139' 'mol2vec_140' 'mol2vec_141' 'mol2vec_142' 'mol2vec_143'\n",
      " 'mol2vec_144' 'mol2vec_145' 'mol2vec_146' 'mol2vec_147' 'mol2vec_148'\n",
      " 'mol2vec_149' 'mol2vec_150' 'mol2vec_151' 'mol2vec_152' 'mol2vec_153'\n",
      " 'mol2vec_154' 'mol2vec_155' 'mol2vec_156' 'mol2vec_157' 'mol2vec_158'\n",
      " 'mol2vec_159' 'mol2vec_160' 'mol2vec_161' 'mol2vec_162' 'mol2vec_163'\n",
      " 'mol2vec_164' 'mol2vec_165' 'mol2vec_166' 'mol2vec_167' 'mol2vec_168'\n",
      " 'mol2vec_169' 'mol2vec_170' 'mol2vec_171' 'mol2vec_172' 'mol2vec_173'\n",
      " 'mol2vec_174' 'mol2vec_175' 'mol2vec_176' 'mol2vec_177' 'mol2vec_178'\n",
      " 'mol2vec_179' 'mol2vec_180' 'mol2vec_181' 'mol2vec_182' 'mol2vec_183'\n",
      " 'mol2vec_184' 'mol2vec_185' 'mol2vec_186' 'mol2vec_187' 'mol2vec_188'\n",
      " 'mol2vec_189' 'mol2vec_190' 'mol2vec_191' 'mol2vec_192' 'mol2vec_193'\n",
      " 'mol2vec_194' 'mol2vec_195' 'mol2vec_196' 'mol2vec_197' 'mol2vec_198'\n",
      " 'mol2vec_199' 'mol2vec_200' 'mol2vec_201' 'mol2vec_202' 'mol2vec_203'\n",
      " 'mol2vec_204' 'mol2vec_205' 'mol2vec_206' 'mol2vec_207' 'mol2vec_208'\n",
      " 'mol2vec_209' 'mol2vec_210' 'mol2vec_211' 'mol2vec_212' 'mol2vec_213'\n",
      " 'mol2vec_214' 'mol2vec_215' 'mol2vec_216' 'mol2vec_217' 'mol2vec_218'\n",
      " 'mol2vec_219' 'mol2vec_220' 'mol2vec_221' 'mol2vec_222' 'mol2vec_223'\n",
      " 'mol2vec_224' 'mol2vec_225' 'mol2vec_226' 'mol2vec_227' 'mol2vec_228'\n",
      " 'mol2vec_229' 'mol2vec_230' 'mol2vec_231' 'mol2vec_232' 'mol2vec_233'\n",
      " 'mol2vec_234' 'mol2vec_235' 'mol2vec_236' 'mol2vec_237' 'mol2vec_238'\n",
      " 'mol2vec_239' 'mol2vec_240' 'mol2vec_241' 'mol2vec_242' 'mol2vec_243'\n",
      " 'mol2vec_244' 'mol2vec_245' 'mol2vec_246' 'mol2vec_247' 'mol2vec_248'\n",
      " 'mol2vec_249' 'mol2vec_250' 'mol2vec_251' 'mol2vec_252' 'mol2vec_253'\n",
      " 'mol2vec_254' 'mol2vec_255' 'mol2vec_256' 'mol2vec_257' 'mol2vec_258'\n",
      " 'mol2vec_259' 'mol2vec_260' 'mol2vec_261' 'mol2vec_262' 'mol2vec_263'\n",
      " 'mol2vec_264' 'mol2vec_265' 'mol2vec_266' 'mol2vec_267' 'mol2vec_268'\n",
      " 'mol2vec_269' 'mol2vec_270' 'mol2vec_271' 'mol2vec_272' 'mol2vec_273'\n",
      " 'mol2vec_274' 'mol2vec_275' 'mol2vec_276' 'mol2vec_277' 'mol2vec_278'\n",
      " 'mol2vec_279' 'mol2vec_280' 'mol2vec_281' 'mol2vec_282' 'mol2vec_283'\n",
      " 'mol2vec_284' 'mol2vec_285' 'mol2vec_286' 'mol2vec_287' 'mol2vec_288'\n",
      " 'mol2vec_289' 'mol2vec_290' 'mol2vec_291' 'mol2vec_292' 'mol2vec_293'\n",
      " 'mol2vec_294' 'mol2vec_295' 'mol2vec_296' 'mol2vec_297' 'mol2vec_298'\n",
      " 'mol2vec_299' 'mol2vec_300']\n",
      "Number of examples in dataset: 1427\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import deepchem as dc\n",
    "\n",
    "\n",
    "current_dir = os.path.dirname(os.path.realpath(\"__file__\"))\n",
    "dataset_file = \"medium_muv.csv.gz\"\n",
    "\n",
    "\n",
    "dataset = dc.utils.save.load_from_disk('/home/mamonteiro/source-code/Project-LEI/SIDER/sider_embeddings.csv')\n",
    "print(\"Columns of dataset: %s\" % str(dataset.columns.values))\n",
    "print(\"Number of examples in dataset: %s\" % str(dataset.shape[0]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The dataset that we must manipulate is thhe follwoing one: dataset\n",
    "\n",
    "In this dataset has as columns the SMILES\n",
    "                               The side efects\n",
    "                               and finally the embeddings\n",
    "        \n",
    "We need to split this dataset in order to provide this data to multitask model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=dataset.drop(['smiles'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hepatobiliary disorders</th>\n",
       "      <th>Metabolism and nutrition disorders</th>\n",
       "      <th>Product issues</th>\n",
       "      <th>Eye disorders</th>\n",
       "      <th>Investigations</th>\n",
       "      <th>Musculoskeletal and connective tissue disorders</th>\n",
       "      <th>Gastrointestinal disorders</th>\n",
       "      <th>Social circumstances</th>\n",
       "      <th>Immune system disorders</th>\n",
       "      <th>Reproductive system and breast disorders</th>\n",
       "      <th>...</th>\n",
       "      <th>mol2vec_291</th>\n",
       "      <th>mol2vec_292</th>\n",
       "      <th>mol2vec_293</th>\n",
       "      <th>mol2vec_294</th>\n",
       "      <th>mol2vec_295</th>\n",
       "      <th>mol2vec_296</th>\n",
       "      <th>mol2vec_297</th>\n",
       "      <th>mol2vec_298</th>\n",
       "      <th>mol2vec_299</th>\n",
       "      <th>mol2vec_300</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.845092</td>\n",
       "      <td>4.080578</td>\n",
       "      <td>5.290233</td>\n",
       "      <td>2.681949</td>\n",
       "      <td>-6.017433</td>\n",
       "      <td>0.732134</td>\n",
       "      <td>-0.161610</td>\n",
       "      <td>-7.355957</td>\n",
       "      <td>-6.514126</td>\n",
       "      <td>-4.416229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.760953</td>\n",
       "      <td>9.614191</td>\n",
       "      <td>13.119958</td>\n",
       "      <td>-0.408570</td>\n",
       "      <td>-7.577562</td>\n",
       "      <td>-4.803534</td>\n",
       "      <td>-4.880173</td>\n",
       "      <td>-7.033062</td>\n",
       "      <td>-15.572207</td>\n",
       "      <td>-4.429869</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 327 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Hepatobiliary disorders  Metabolism and nutrition disorders  \\\n",
       "0                        1                                   1   \n",
       "1                        0                                   1   \n",
       "\n",
       "   Product issues  Eye disorders  Investigations  \\\n",
       "0               0              0               1   \n",
       "1               0              0               1   \n",
       "\n",
       "   Musculoskeletal and connective tissue disorders  \\\n",
       "0                                                1   \n",
       "1                                                1   \n",
       "\n",
       "   Gastrointestinal disorders  Social circumstances  Immune system disorders  \\\n",
       "0                           1                     0                        0   \n",
       "1                           1                     0                        0   \n",
       "\n",
       "   Reproductive system and breast disorders  ...  mol2vec_291  mol2vec_292  \\\n",
       "0                                         0  ...     1.845092     4.080578   \n",
       "1                                         1  ...    -0.760953     9.614191   \n",
       "\n",
       "   mol2vec_293  mol2vec_294  mol2vec_295  mol2vec_296  mol2vec_297  \\\n",
       "0     5.290233     2.681949    -6.017433     0.732134    -0.161610   \n",
       "1    13.119958    -0.408570    -7.577562    -4.803534    -4.880173   \n",
       "\n",
       "   mol2vec_298  mol2vec_299  mol2vec_300  \n",
       "0    -7.355957    -6.514126    -4.416229  \n",
       "1    -7.033062   -15.572207    -4.429869  \n",
       "\n",
       "[2 rows x 327 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hepatobiliary disorders</th>\n",
       "      <th>Metabolism and nutrition disorders</th>\n",
       "      <th>Product issues</th>\n",
       "      <th>Eye disorders</th>\n",
       "      <th>Investigations</th>\n",
       "      <th>Musculoskeletal and connective tissue disorders</th>\n",
       "      <th>Gastrointestinal disorders</th>\n",
       "      <th>Social circumstances</th>\n",
       "      <th>Immune system disorders</th>\n",
       "      <th>Reproductive system and breast disorders</th>\n",
       "      <th>...</th>\n",
       "      <th>Congenital, familial and genetic disorders</th>\n",
       "      <th>Infections and infestations</th>\n",
       "      <th>Respiratory, thoracic and mediastinal disorders</th>\n",
       "      <th>Psychiatric disorders</th>\n",
       "      <th>Renal and urinary disorders</th>\n",
       "      <th>Pregnancy, puerperium and perinatal conditions</th>\n",
       "      <th>Ear and labyrinth disorders</th>\n",
       "      <th>Cardiac disorders</th>\n",
       "      <th>Nervous system disorders</th>\n",
       "      <th>Injury, poisoning and procedural complications</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Hepatobiliary disorders  Metabolism and nutrition disorders  \\\n",
       "0                        1                                   1   \n",
       "1                        0                                   1   \n",
       "2                        0                                   1   \n",
       "3                        1                                   1   \n",
       "4                        1                                   1   \n",
       "\n",
       "   Product issues  Eye disorders  Investigations  \\\n",
       "0               0              0               1   \n",
       "1               0              0               1   \n",
       "2               0              1               1   \n",
       "3               0              1               1   \n",
       "4               0              1               1   \n",
       "\n",
       "   Musculoskeletal and connective tissue disorders  \\\n",
       "0                                                1   \n",
       "1                                                1   \n",
       "2                                                0   \n",
       "3                                                1   \n",
       "4                                                1   \n",
       "\n",
       "   Gastrointestinal disorders  Social circumstances  Immune system disorders  \\\n",
       "0                           1                     0                        0   \n",
       "1                           1                     0                        0   \n",
       "2                           1                     0                        1   \n",
       "3                           1                     0                        1   \n",
       "4                           1                     0                        1   \n",
       "\n",
       "   Reproductive system and breast disorders  ...  \\\n",
       "0                                         0  ...   \n",
       "1                                         1  ...   \n",
       "2                                         1  ...   \n",
       "3                                         1  ...   \n",
       "4                                         0  ...   \n",
       "\n",
       "   Congenital, familial and genetic disorders  Infections and infestations  \\\n",
       "0                                           0                            0   \n",
       "1                                           0                            1   \n",
       "2                                           0                            0   \n",
       "3                                           1                            1   \n",
       "4                                           0                            1   \n",
       "\n",
       "   Respiratory, thoracic and mediastinal disorders  Psychiatric disorders  \\\n",
       "0                                                1                      1   \n",
       "1                                                1                      0   \n",
       "2                                                0                      1   \n",
       "3                                                1                      1   \n",
       "4                                                1                      1   \n",
       "\n",
       "   Renal and urinary disorders  \\\n",
       "0                            0   \n",
       "1                            0   \n",
       "2                            0   \n",
       "3                            1   \n",
       "4                            0   \n",
       "\n",
       "   Pregnancy, puerperium and perinatal conditions  \\\n",
       "0                                               0   \n",
       "1                                               0   \n",
       "2                                               0   \n",
       "3                                               1   \n",
       "4                                               0   \n",
       "\n",
       "   Ear and labyrinth disorders  Cardiac disorders  Nervous system disorders  \\\n",
       "0                            1                  1                         1   \n",
       "1                            1                  0                         1   \n",
       "2                            0                  0                         1   \n",
       "3                            0                  0                         1   \n",
       "4                            1                  0                         1   \n",
       "\n",
       "   Injury, poisoning and procedural complications  \n",
       "0                                               0  \n",
       "1                                               0  \n",
       "2                                               0  \n",
       "3                                               1  \n",
       "4                                               0  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sideEfects=\n",
    "dataset.iloc[:,0:27].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "sideEfects= dataset.iloc[:,0:27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hepatobiliary disorders</th>\n",
       "      <th>Metabolism and nutrition disorders</th>\n",
       "      <th>Product issues</th>\n",
       "      <th>Eye disorders</th>\n",
       "      <th>Investigations</th>\n",
       "      <th>Musculoskeletal and connective tissue disorders</th>\n",
       "      <th>Gastrointestinal disorders</th>\n",
       "      <th>Social circumstances</th>\n",
       "      <th>Immune system disorders</th>\n",
       "      <th>Reproductive system and breast disorders</th>\n",
       "      <th>...</th>\n",
       "      <th>Congenital, familial and genetic disorders</th>\n",
       "      <th>Infections and infestations</th>\n",
       "      <th>Respiratory, thoracic and mediastinal disorders</th>\n",
       "      <th>Psychiatric disorders</th>\n",
       "      <th>Renal and urinary disorders</th>\n",
       "      <th>Pregnancy, puerperium and perinatal conditions</th>\n",
       "      <th>Ear and labyrinth disorders</th>\n",
       "      <th>Cardiac disorders</th>\n",
       "      <th>Nervous system disorders</th>\n",
       "      <th>Injury, poisoning and procedural complications</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Hepatobiliary disorders  Metabolism and nutrition disorders  \\\n",
       "0                        1                                   1   \n",
       "1                        0                                   1   \n",
       "2                        0                                   1   \n",
       "3                        1                                   1   \n",
       "4                        1                                   1   \n",
       "\n",
       "   Product issues  Eye disorders  Investigations  \\\n",
       "0               0              0               1   \n",
       "1               0              0               1   \n",
       "2               0              1               1   \n",
       "3               0              1               1   \n",
       "4               0              1               1   \n",
       "\n",
       "   Musculoskeletal and connective tissue disorders  \\\n",
       "0                                                1   \n",
       "1                                                1   \n",
       "2                                                0   \n",
       "3                                                1   \n",
       "4                                                1   \n",
       "\n",
       "   Gastrointestinal disorders  Social circumstances  Immune system disorders  \\\n",
       "0                           1                     0                        0   \n",
       "1                           1                     0                        0   \n",
       "2                           1                     0                        1   \n",
       "3                           1                     0                        1   \n",
       "4                           1                     0                        1   \n",
       "\n",
       "   Reproductive system and breast disorders  ...  \\\n",
       "0                                         0  ...   \n",
       "1                                         1  ...   \n",
       "2                                         1  ...   \n",
       "3                                         1  ...   \n",
       "4                                         0  ...   \n",
       "\n",
       "   Congenital, familial and genetic disorders  Infections and infestations  \\\n",
       "0                                           0                            0   \n",
       "1                                           0                            1   \n",
       "2                                           0                            0   \n",
       "3                                           1                            1   \n",
       "4                                           0                            1   \n",
       "\n",
       "   Respiratory, thoracic and mediastinal disorders  Psychiatric disorders  \\\n",
       "0                                                1                      1   \n",
       "1                                                1                      0   \n",
       "2                                                0                      1   \n",
       "3                                                1                      1   \n",
       "4                                                1                      1   \n",
       "\n",
       "   Renal and urinary disorders  \\\n",
       "0                            0   \n",
       "1                            0   \n",
       "2                            0   \n",
       "3                            1   \n",
       "4                            0   \n",
       "\n",
       "   Pregnancy, puerperium and perinatal conditions  \\\n",
       "0                                               0   \n",
       "1                                               0   \n",
       "2                                               0   \n",
       "3                                               1   \n",
       "4                                               0   \n",
       "\n",
       "   Ear and labyrinth disorders  Cardiac disorders  Nervous system disorders  \\\n",
       "0                            1                  1                         1   \n",
       "1                            1                  0                         1   \n",
       "2                            0                  0                         1   \n",
       "3                            0                  0                         1   \n",
       "4                            1                  0                         1   \n",
       "\n",
       "   Injury, poisoning and procedural complications  \n",
       "0                                               0  \n",
       "1                                               0  \n",
       "2                                               0  \n",
       "3                                               1  \n",
       "4                                               0  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sideEfects.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings=dataset.iloc[:,27:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mol2vec_1</th>\n",
       "      <th>mol2vec_2</th>\n",
       "      <th>mol2vec_3</th>\n",
       "      <th>mol2vec_4</th>\n",
       "      <th>mol2vec_5</th>\n",
       "      <th>mol2vec_6</th>\n",
       "      <th>mol2vec_7</th>\n",
       "      <th>mol2vec_8</th>\n",
       "      <th>mol2vec_9</th>\n",
       "      <th>mol2vec_10</th>\n",
       "      <th>...</th>\n",
       "      <th>mol2vec_291</th>\n",
       "      <th>mol2vec_292</th>\n",
       "      <th>mol2vec_293</th>\n",
       "      <th>mol2vec_294</th>\n",
       "      <th>mol2vec_295</th>\n",
       "      <th>mol2vec_296</th>\n",
       "      <th>mol2vec_297</th>\n",
       "      <th>mol2vec_298</th>\n",
       "      <th>mol2vec_299</th>\n",
       "      <th>mol2vec_300</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.990727</td>\n",
       "      <td>-1.723967</td>\n",
       "      <td>1.596080</td>\n",
       "      <td>0.336589</td>\n",
       "      <td>5.995870</td>\n",
       "      <td>1.602312</td>\n",
       "      <td>-7.893780</td>\n",
       "      <td>-0.770941</td>\n",
       "      <td>2.798226</td>\n",
       "      <td>-4.712134</td>\n",
       "      <td>...</td>\n",
       "      <td>1.845092</td>\n",
       "      <td>4.080578</td>\n",
       "      <td>5.290233</td>\n",
       "      <td>2.681949</td>\n",
       "      <td>-6.017433</td>\n",
       "      <td>0.732134</td>\n",
       "      <td>-0.161610</td>\n",
       "      <td>-7.355957</td>\n",
       "      <td>-6.514126</td>\n",
       "      <td>-4.416229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.596306</td>\n",
       "      <td>0.060384</td>\n",
       "      <td>-4.686247</td>\n",
       "      <td>4.381831</td>\n",
       "      <td>2.139633</td>\n",
       "      <td>-0.343262</td>\n",
       "      <td>-13.849467</td>\n",
       "      <td>0.780369</td>\n",
       "      <td>9.671047</td>\n",
       "      <td>4.445226</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.760953</td>\n",
       "      <td>9.614191</td>\n",
       "      <td>13.119958</td>\n",
       "      <td>-0.408570</td>\n",
       "      <td>-7.577562</td>\n",
       "      <td>-4.803534</td>\n",
       "      <td>-4.880173</td>\n",
       "      <td>-7.033062</td>\n",
       "      <td>-15.572207</td>\n",
       "      <td>-4.429869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.854099</td>\n",
       "      <td>-4.819261</td>\n",
       "      <td>0.775984</td>\n",
       "      <td>3.286393</td>\n",
       "      <td>1.519570</td>\n",
       "      <td>-6.281527</td>\n",
       "      <td>-10.383826</td>\n",
       "      <td>5.000489</td>\n",
       "      <td>-0.742258</td>\n",
       "      <td>0.579260</td>\n",
       "      <td>...</td>\n",
       "      <td>0.873612</td>\n",
       "      <td>15.422496</td>\n",
       "      <td>-1.348580</td>\n",
       "      <td>-1.571692</td>\n",
       "      <td>-17.224537</td>\n",
       "      <td>-6.963869</td>\n",
       "      <td>-12.192692</td>\n",
       "      <td>-3.912728</td>\n",
       "      <td>-8.017673</td>\n",
       "      <td>1.265650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.140168</td>\n",
       "      <td>-5.169839</td>\n",
       "      <td>-0.039611</td>\n",
       "      <td>3.000724</td>\n",
       "      <td>0.787211</td>\n",
       "      <td>-6.880911</td>\n",
       "      <td>-10.384640</td>\n",
       "      <td>5.767844</td>\n",
       "      <td>-0.478307</td>\n",
       "      <td>0.191780</td>\n",
       "      <td>...</td>\n",
       "      <td>0.627151</td>\n",
       "      <td>15.921182</td>\n",
       "      <td>-1.159732</td>\n",
       "      <td>-1.260444</td>\n",
       "      <td>-17.655016</td>\n",
       "      <td>-7.629984</td>\n",
       "      <td>-13.157971</td>\n",
       "      <td>-3.574889</td>\n",
       "      <td>-8.912206</td>\n",
       "      <td>1.509292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.227203</td>\n",
       "      <td>-2.800524</td>\n",
       "      <td>-1.099673</td>\n",
       "      <td>4.259158</td>\n",
       "      <td>-1.261016</td>\n",
       "      <td>-2.570959</td>\n",
       "      <td>-8.051775</td>\n",
       "      <td>1.025365</td>\n",
       "      <td>6.135835</td>\n",
       "      <td>2.401863</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.972749</td>\n",
       "      <td>9.117423</td>\n",
       "      <td>7.184863</td>\n",
       "      <td>-0.012285</td>\n",
       "      <td>-7.312431</td>\n",
       "      <td>-1.483516</td>\n",
       "      <td>-4.117091</td>\n",
       "      <td>-2.378627</td>\n",
       "      <td>-9.008883</td>\n",
       "      <td>-0.717169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mol2vec_1  mol2vec_2  mol2vec_3  mol2vec_4  mol2vec_5  mol2vec_6  \\\n",
       "0  -0.990727  -1.723967   1.596080   0.336589   5.995870   1.602312   \n",
       "1   0.596306   0.060384  -4.686247   4.381831   2.139633  -0.343262   \n",
       "2   1.854099  -4.819261   0.775984   3.286393   1.519570  -6.281527   \n",
       "3   2.140168  -5.169839  -0.039611   3.000724   0.787211  -6.880911   \n",
       "4  -0.227203  -2.800524  -1.099673   4.259158  -1.261016  -2.570959   \n",
       "\n",
       "   mol2vec_7  mol2vec_8  mol2vec_9  mol2vec_10  ...  mol2vec_291  mol2vec_292  \\\n",
       "0  -7.893780  -0.770941   2.798226   -4.712134  ...     1.845092     4.080578   \n",
       "1 -13.849467   0.780369   9.671047    4.445226  ...    -0.760953     9.614191   \n",
       "2 -10.383826   5.000489  -0.742258    0.579260  ...     0.873612    15.422496   \n",
       "3 -10.384640   5.767844  -0.478307    0.191780  ...     0.627151    15.921182   \n",
       "4  -8.051775   1.025365   6.135835    2.401863  ...    -0.972749     9.117423   \n",
       "\n",
       "   mol2vec_293  mol2vec_294  mol2vec_295  mol2vec_296  mol2vec_297  \\\n",
       "0     5.290233     2.681949    -6.017433     0.732134    -0.161610   \n",
       "1    13.119958    -0.408570    -7.577562    -4.803534    -4.880173   \n",
       "2    -1.348580    -1.571692   -17.224537    -6.963869   -12.192692   \n",
       "3    -1.159732    -1.260444   -17.655016    -7.629984   -13.157971   \n",
       "4     7.184863    -0.012285    -7.312431    -1.483516    -4.117091   \n",
       "\n",
       "   mol2vec_298  mol2vec_299  mol2vec_300  \n",
       "0    -7.355957    -6.514126    -4.416229  \n",
       "1    -7.033062   -15.572207    -4.429869  \n",
       "2    -3.912728    -8.017673     1.265650  \n",
       "3    -3.574889    -8.912206     1.509292  \n",
       "4    -2.378627    -9.008883    -0.717169  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.isnull().any().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sideEfects.isnull().any().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling and Principal component analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "NCA1 = 100\n",
    "NCA2 = 100\n",
    "DROPRATE = 0.2\n",
    "EP = 500\n",
    "BATCH_SIZE = 128\n",
    "VAL_RATIO = 0.1\n",
    "TEST_RATIO = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from skmultilearn.model_selection import iterative_train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = iterative_train_test_split(embeddings.values,\n",
    "                                                              sideEfects.values, \n",
    "                                                              test_size=TEST_RATIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_valid, y_valid = iterative_train_test_split(X_train, y_train, \n",
    "                                                                test_size=VAL_RATIO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Dropout, BatchNormalization, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "sider_model = Sequential()\n",
    "sider_model.add(Dense(128, input_dim=embeddings.shape[1], \n",
    "                      kernel_initializer='he_uniform'))\n",
    "sider_model.add(BatchNormalization())\n",
    "sider_model.add(Activation('tanh'))\n",
    "sider_model.add(Dropout(rate=DROPRATE))\n",
    "sider_model.add(Dense(64,kernel_initializer='he_uniform'))\n",
    "sider_model.add(BatchNormalization())\n",
    "sider_model.add(Activation('tanh'))\n",
    "sider_model.add(Dropout(rate=DROPRATE))\n",
    "sider_model.add(Dense(32,kernel_initializer='he_uniform'))\n",
    "sider_model.add(BatchNormalization())\n",
    "sider_model.add(Activation('tanh'))\n",
    "sider_model.add(Dropout(rate=DROPRATE))\n",
    "sider_model.add(Dense(sideEfects.shape[1],kernel_initializer='he_uniform',activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "sider_model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('sider_model.h5', monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True, mode='min')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mamonteiro/anaconda3/envs/lei/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mamonteiro/anaconda3/envs/lei/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1125 samples, validate on 151 samples\n",
      "Epoch 1/500\n",
      "1125/1125 [==============================] - 2s 2ms/step - loss: 0.7790 - acc: 0.5037 - val_loss: 0.7283 - val_acc: 0.5065\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.72834, saving model to sider_model.h5\n",
      "Epoch 2/500\n",
      "1125/1125 [==============================] - 0s 69us/step - loss: 0.7433 - acc: 0.5211 - val_loss: 0.7074 - val_acc: 0.5372\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.72834 to 0.70741, saving model to sider_model.h5\n",
      "Epoch 3/500\n",
      "1125/1125 [==============================] - 0s 72us/step - loss: 0.7251 - acc: 0.5344 - val_loss: 0.6935 - val_acc: 0.5570\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.70741 to 0.69347, saving model to sider_model.h5\n",
      "Epoch 4/500\n",
      "1125/1125 [==============================] - 0s 67us/step - loss: 0.7116 - acc: 0.5454 - val_loss: 0.6856 - val_acc: 0.5695\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.69347 to 0.68564, saving model to sider_model.h5\n",
      "Epoch 5/500\n",
      "1125/1125 [==============================] - 0s 83us/step - loss: 0.6983 - acc: 0.5572 - val_loss: 0.6765 - val_acc: 0.5872\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.68564 to 0.67655, saving model to sider_model.h5\n",
      "Epoch 6/500\n",
      "1125/1125 [==============================] - 0s 71us/step - loss: 0.6930 - acc: 0.5618 - val_loss: 0.6663 - val_acc: 0.6085\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.67655 to 0.66634, saving model to sider_model.h5\n",
      "Epoch 7/500\n",
      "1125/1125 [==============================] - 0s 80us/step - loss: 0.6835 - acc: 0.5727 - val_loss: 0.6584 - val_acc: 0.6233\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.66634 to 0.65840, saving model to sider_model.h5\n",
      "Epoch 8/500\n",
      "1125/1125 [==============================] - 0s 88us/step - loss: 0.6749 - acc: 0.5864 - val_loss: 0.6576 - val_acc: 0.6282\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.65840 to 0.65763, saving model to sider_model.h5\n",
      "Epoch 9/500\n",
      "1125/1125 [==============================] - 0s 79us/step - loss: 0.6689 - acc: 0.5935 - val_loss: 0.6527 - val_acc: 0.6355\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.65763 to 0.65266, saving model to sider_model.h5\n",
      "Epoch 10/500\n",
      "1125/1125 [==============================] - 0s 78us/step - loss: 0.6611 - acc: 0.6077 - val_loss: 0.6435 - val_acc: 0.6463\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.65266 to 0.64347, saving model to sider_model.h5\n",
      "Epoch 11/500\n",
      "1125/1125 [==============================] - 0s 71us/step - loss: 0.6538 - acc: 0.6153 - val_loss: 0.6432 - val_acc: 0.6493\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.64347 to 0.64322, saving model to sider_model.h5\n",
      "Epoch 12/500\n",
      "1125/1125 [==============================] - 0s 73us/step - loss: 0.6421 - acc: 0.6353 - val_loss: 0.6418 - val_acc: 0.6507\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.64322 to 0.64177, saving model to sider_model.h5\n",
      "Epoch 13/500\n",
      "1125/1125 [==============================] - 0s 71us/step - loss: 0.6340 - acc: 0.6462 - val_loss: 0.6388 - val_acc: 0.6495\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.64177 to 0.63879, saving model to sider_model.h5\n",
      "Epoch 14/500\n",
      "1125/1125 [==============================] - 0s 77us/step - loss: 0.6273 - acc: 0.6531 - val_loss: 0.6245 - val_acc: 0.6703\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.63879 to 0.62451, saving model to sider_model.h5\n",
      "Epoch 15/500\n",
      "1125/1125 [==============================] - 0s 69us/step - loss: 0.6221 - acc: 0.6601 - val_loss: 0.6251 - val_acc: 0.6726\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.62451\n",
      "Epoch 16/500\n",
      "1125/1125 [==============================] - 0s 88us/step - loss: 0.6092 - acc: 0.6762 - val_loss: 0.6117 - val_acc: 0.6841\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.62451 to 0.61167, saving model to sider_model.h5\n",
      "Epoch 17/500\n",
      "1125/1125 [==============================] - 0s 77us/step - loss: 0.6018 - acc: 0.6842 - val_loss: 0.6092 - val_acc: 0.6860\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.61167 to 0.60915, saving model to sider_model.h5\n",
      "Epoch 18/500\n",
      "1125/1125 [==============================] - 0s 69us/step - loss: 0.5915 - acc: 0.6945 - val_loss: 0.6025 - val_acc: 0.6956\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.60915 to 0.60248, saving model to sider_model.h5\n",
      "Epoch 19/500\n",
      "1125/1125 [==============================] - 0s 85us/step - loss: 0.5829 - acc: 0.7042 - val_loss: 0.5918 - val_acc: 0.7076\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.60248 to 0.59183, saving model to sider_model.h5\n",
      "Epoch 20/500\n",
      "1125/1125 [==============================] - 0s 71us/step - loss: 0.5729 - acc: 0.7138 - val_loss: 0.5860 - val_acc: 0.7118\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.59183 to 0.58603, saving model to sider_model.h5\n",
      "Epoch 21/500\n",
      "1125/1125 [==============================] - 0s 72us/step - loss: 0.5642 - acc: 0.7206 - val_loss: 0.5808 - val_acc: 0.7106\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.58603 to 0.58078, saving model to sider_model.h5\n",
      "Epoch 22/500\n",
      "1125/1125 [==============================] - 0s 73us/step - loss: 0.5515 - acc: 0.7289 - val_loss: 0.5753 - val_acc: 0.7165\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.58078 to 0.57532, saving model to sider_model.h5\n",
      "Epoch 23/500\n",
      "1125/1125 [==============================] - 0s 79us/step - loss: 0.5472 - acc: 0.7354 - val_loss: 0.5748 - val_acc: 0.7098\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.57532 to 0.57484, saving model to sider_model.h5\n",
      "Epoch 24/500\n",
      "1125/1125 [==============================] - 0s 65us/step - loss: 0.5418 - acc: 0.7391 - val_loss: 0.5676 - val_acc: 0.7187\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.57484 to 0.56757, saving model to sider_model.h5\n",
      "Epoch 25/500\n",
      "1125/1125 [==============================] - 0s 77us/step - loss: 0.5291 - acc: 0.7482 - val_loss: 0.5662 - val_acc: 0.7196\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.56757 to 0.56623, saving model to sider_model.h5\n",
      "Epoch 26/500\n",
      "1125/1125 [==============================] - 0s 64us/step - loss: 0.5255 - acc: 0.7508 - val_loss: 0.5618 - val_acc: 0.7253\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.56623 to 0.56181, saving model to sider_model.h5\n",
      "Epoch 27/500\n",
      "1125/1125 [==============================] - 0s 79us/step - loss: 0.5222 - acc: 0.7514 - val_loss: 0.5563 - val_acc: 0.7292\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.56181 to 0.55628, saving model to sider_model.h5\n",
      "Epoch 28/500\n",
      "1125/1125 [==============================] - 0s 79us/step - loss: 0.5130 - acc: 0.7593 - val_loss: 0.5523 - val_acc: 0.7290\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.55628 to 0.55228, saving model to sider_model.h5\n",
      "Epoch 29/500\n",
      "1125/1125 [==============================] - 0s 84us/step - loss: 0.5047 - acc: 0.7637 - val_loss: 0.5464 - val_acc: 0.7356\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.55228 to 0.54639, saving model to sider_model.h5\n",
      "Epoch 30/500\n",
      "1125/1125 [==============================] - 0s 74us/step - loss: 0.5071 - acc: 0.7615 - val_loss: 0.5440 - val_acc: 0.7349\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.54639 to 0.54400, saving model to sider_model.h5\n",
      "Epoch 31/500\n",
      "1125/1125 [==============================] - 0s 76us/step - loss: 0.4975 - acc: 0.7673 - val_loss: 0.5442 - val_acc: 0.7334\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.54400\n",
      "Epoch 32/500\n",
      "1125/1125 [==============================] - 0s 74us/step - loss: 0.4969 - acc: 0.7672 - val_loss: 0.5480 - val_acc: 0.7331\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.54400\n",
      "Epoch 33/500\n",
      "1125/1125 [==============================] - 0s 74us/step - loss: 0.4920 - acc: 0.7698 - val_loss: 0.5427 - val_acc: 0.7334\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.54400 to 0.54269, saving model to sider_model.h5\n",
      "Epoch 34/500\n",
      "1125/1125 [==============================] - 0s 78us/step - loss: 0.4914 - acc: 0.7694 - val_loss: 0.5355 - val_acc: 0.7356\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.54269 to 0.53554, saving model to sider_model.h5\n",
      "Epoch 35/500\n",
      "1125/1125 [==============================] - 0s 79us/step - loss: 0.4879 - acc: 0.7723 - val_loss: 0.5530 - val_acc: 0.7299\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.53554\n",
      "Epoch 36/500\n",
      "1125/1125 [==============================] - 0s 95us/step - loss: 0.4892 - acc: 0.7705 - val_loss: 0.5371 - val_acc: 0.7331\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.53554\n",
      "Epoch 37/500\n",
      "1125/1125 [==============================] - 0s 77us/step - loss: 0.4800 - acc: 0.7757 - val_loss: 0.5402 - val_acc: 0.7324\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.53554\n",
      "Epoch 38/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1125/1125 [==============================] - 0s 86us/step - loss: 0.4806 - acc: 0.7739 - val_loss: 0.5358 - val_acc: 0.7346\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.53554\n",
      "Epoch 39/500\n",
      "1125/1125 [==============================] - 0s 88us/step - loss: 0.4804 - acc: 0.7729 - val_loss: 0.5337 - val_acc: 0.7361\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.53554 to 0.53372, saving model to sider_model.h5\n",
      "Epoch 40/500\n",
      "1125/1125 [==============================] - 0s 64us/step - loss: 0.4768 - acc: 0.7763 - val_loss: 0.5472 - val_acc: 0.7302\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.53372\n",
      "Epoch 41/500\n",
      "1125/1125 [==============================] - 0s 83us/step - loss: 0.4720 - acc: 0.7784 - val_loss: 0.5556 - val_acc: 0.7309\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.53372\n",
      "Epoch 42/500\n",
      "1125/1125 [==============================] - 0s 85us/step - loss: 0.4747 - acc: 0.7779 - val_loss: 0.5503 - val_acc: 0.7326\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.53372\n",
      "Epoch 43/500\n",
      "1125/1125 [==============================] - 0s 72us/step - loss: 0.4737 - acc: 0.7785 - val_loss: 0.5366 - val_acc: 0.7358\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.53372\n",
      "Epoch 44/500\n",
      "1125/1125 [==============================] - 0s 77us/step - loss: 0.4698 - acc: 0.7789 - val_loss: 0.5305 - val_acc: 0.7358\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.53372 to 0.53048, saving model to sider_model.h5\n",
      "Epoch 45/500\n",
      "1125/1125 [==============================] - 0s 74us/step - loss: 0.4691 - acc: 0.7820 - val_loss: 0.5502 - val_acc: 0.7324\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.53048\n",
      "Epoch 46/500\n",
      "1125/1125 [==============================] - 0s 85us/step - loss: 0.4694 - acc: 0.7809 - val_loss: 0.5465 - val_acc: 0.7312\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.53048\n",
      "Epoch 47/500\n",
      "1125/1125 [==============================] - 0s 79us/step - loss: 0.4673 - acc: 0.7802 - val_loss: 0.5393 - val_acc: 0.7299\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.53048\n",
      "Epoch 48/500\n",
      "1125/1125 [==============================] - 0s 62us/step - loss: 0.4644 - acc: 0.7846 - val_loss: 0.5366 - val_acc: 0.7329\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.53048\n",
      "Epoch 49/500\n",
      "1125/1125 [==============================] - 0s 77us/step - loss: 0.4643 - acc: 0.7828 - val_loss: 0.5515 - val_acc: 0.7317\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.53048\n",
      "Epoch 50/500\n",
      "1125/1125 [==============================] - 0s 75us/step - loss: 0.4646 - acc: 0.7833 - val_loss: 0.5357 - val_acc: 0.7304\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.53048\n",
      "Epoch 51/500\n",
      "1125/1125 [==============================] - 0s 72us/step - loss: 0.4624 - acc: 0.7843 - val_loss: 0.5409 - val_acc: 0.7282\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.53048\n",
      "Epoch 52/500\n",
      "1125/1125 [==============================] - 0s 79us/step - loss: 0.4604 - acc: 0.7842 - val_loss: 0.5298 - val_acc: 0.7378\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.53048 to 0.52977, saving model to sider_model.h5\n",
      "Epoch 53/500\n",
      "1125/1125 [==============================] - 0s 79us/step - loss: 0.4561 - acc: 0.7857 - val_loss: 0.5900 - val_acc: 0.7179\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.52977\n",
      "Epoch 54/500\n",
      "1125/1125 [==============================] - 0s 92us/step - loss: 0.4620 - acc: 0.7833 - val_loss: 0.5413 - val_acc: 0.7324\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.52977\n",
      "Epoch 55/500\n",
      "1125/1125 [==============================] - 0s 77us/step - loss: 0.4620 - acc: 0.7826 - val_loss: 0.5299 - val_acc: 0.7380\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.52977\n",
      "Epoch 56/500\n",
      "1125/1125 [==============================] - 0s 79us/step - loss: 0.4615 - acc: 0.7813 - val_loss: 0.5723 - val_acc: 0.7260\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.52977\n",
      "Epoch 57/500\n",
      "1125/1125 [==============================] - 0s 66us/step - loss: 0.4615 - acc: 0.7826 - val_loss: 0.5476 - val_acc: 0.7317\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.52977\n",
      "Epoch 58/500\n",
      "1125/1125 [==============================] - 0s 72us/step - loss: 0.4585 - acc: 0.7849 - val_loss: 0.5356 - val_acc: 0.7339\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.52977\n",
      "Epoch 59/500\n",
      "1125/1125 [==============================] - 0s 88us/step - loss: 0.4546 - acc: 0.7903 - val_loss: 0.5374 - val_acc: 0.7341\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.52977\n",
      "Epoch 60/500\n",
      "1125/1125 [==============================] - 0s 86us/step - loss: 0.4531 - acc: 0.7893 - val_loss: 0.5609 - val_acc: 0.7223\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.52977\n",
      "Epoch 61/500\n",
      "1125/1125 [==============================] - 0s 93us/step - loss: 0.4524 - acc: 0.7893 - val_loss: 0.5303 - val_acc: 0.7390\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.52977\n",
      "Epoch 62/500\n",
      "1125/1125 [==============================] - 0s 68us/step - loss: 0.4512 - acc: 0.7893 - val_loss: 0.5400 - val_acc: 0.7346\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.52977\n",
      "Epoch 63/500\n",
      "1125/1125 [==============================] - 0s 70us/step - loss: 0.4494 - acc: 0.7887 - val_loss: 0.5579 - val_acc: 0.7292\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.52977\n",
      "Epoch 64/500\n",
      "1125/1125 [==============================] - 0s 91us/step - loss: 0.4550 - acc: 0.7876 - val_loss: 0.5595 - val_acc: 0.7292\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.52977\n",
      "Epoch 65/500\n",
      "1125/1125 [==============================] - 0s 74us/step - loss: 0.4491 - acc: 0.7914 - val_loss: 0.5599 - val_acc: 0.7304\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.52977\n",
      "Epoch 66/500\n",
      "1125/1125 [==============================] - 0s 71us/step - loss: 0.4513 - acc: 0.7893 - val_loss: 0.5448 - val_acc: 0.7336\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.52977\n",
      "Epoch 67/500\n",
      "1125/1125 [==============================] - 0s 67us/step - loss: 0.4470 - acc: 0.7917 - val_loss: 0.5663 - val_acc: 0.7307\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.52977\n",
      "Epoch 68/500\n",
      "1125/1125 [==============================] - 0s 77us/step - loss: 0.4530 - acc: 0.7874 - val_loss: 0.5629 - val_acc: 0.7307\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.52977\n",
      "Epoch 69/500\n",
      "1125/1125 [==============================] - 0s 72us/step - loss: 0.4486 - acc: 0.7911 - val_loss: 0.5404 - val_acc: 0.7353\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.52977\n",
      "Epoch 70/500\n",
      "1125/1125 [==============================] - 0s 85us/step - loss: 0.4453 - acc: 0.7898 - val_loss: 0.5442 - val_acc: 0.7319\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.52977\n",
      "Epoch 71/500\n",
      "1125/1125 [==============================] - 0s 73us/step - loss: 0.4440 - acc: 0.7950 - val_loss: 0.5572 - val_acc: 0.7285\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.52977\n",
      "Epoch 72/500\n",
      "1125/1125 [==============================] - 0s 78us/step - loss: 0.4415 - acc: 0.7953 - val_loss: 0.5444 - val_acc: 0.7317\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.52977\n",
      "Epoch 73/500\n",
      "1125/1125 [==============================] - 0s 77us/step - loss: 0.4421 - acc: 0.7937 - val_loss: 0.5649 - val_acc: 0.7260\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.52977\n",
      "Epoch 74/500\n",
      "1125/1125 [==============================] - 0s 100us/step - loss: 0.4419 - acc: 0.7963 - val_loss: 0.5534 - val_acc: 0.7322\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.52977\n",
      "Epoch 75/500\n",
      "1125/1125 [==============================] - 0s 73us/step - loss: 0.4404 - acc: 0.7953 - val_loss: 0.5676 - val_acc: 0.7280\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.52977\n",
      "Epoch 76/500\n",
      "1125/1125 [==============================] - 0s 83us/step - loss: 0.4378 - acc: 0.7972 - val_loss: 0.5538 - val_acc: 0.7317\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.52977\n",
      "Epoch 77/500\n",
      "1125/1125 [==============================] - 0s 73us/step - loss: 0.4402 - acc: 0.7936 - val_loss: 0.5385 - val_acc: 0.7275\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.52977\n",
      "Epoch 78/500\n",
      "1125/1125 [==============================] - 0s 83us/step - loss: 0.4419 - acc: 0.7931 - val_loss: 0.5690 - val_acc: 0.7243\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.52977\n",
      "Epoch 79/500\n",
      "1125/1125 [==============================] - 0s 77us/step - loss: 0.4433 - acc: 0.7935 - val_loss: 0.5816 - val_acc: 0.7243\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.52977\n",
      "Epoch 80/500\n",
      "1125/1125 [==============================] - 0s 76us/step - loss: 0.4397 - acc: 0.7934 - val_loss: 0.5872 - val_acc: 0.7206\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.52977\n",
      "Epoch 81/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1125/1125 [==============================] - 0s 85us/step - loss: 0.4365 - acc: 0.7978 - val_loss: 0.5387 - val_acc: 0.7287\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.52977\n",
      "Epoch 82/500\n",
      "1125/1125 [==============================] - 0s 69us/step - loss: 0.4400 - acc: 0.7948 - val_loss: 0.5449 - val_acc: 0.7331\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.52977\n",
      "Epoch 83/500\n",
      "1125/1125 [==============================] - 0s 84us/step - loss: 0.4386 - acc: 0.7958 - val_loss: 0.5481 - val_acc: 0.7334\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.52977\n",
      "Epoch 84/500\n",
      "1125/1125 [==============================] - 0s 76us/step - loss: 0.4407 - acc: 0.7958 - val_loss: 0.5876 - val_acc: 0.7241\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.52977\n",
      "Epoch 85/500\n",
      "1125/1125 [==============================] - 0s 78us/step - loss: 0.4366 - acc: 0.7964 - val_loss: 0.5603 - val_acc: 0.7238\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.52977\n",
      "Epoch 86/500\n",
      "1125/1125 [==============================] - 0s 75us/step - loss: 0.4353 - acc: 0.7963 - val_loss: 0.5442 - val_acc: 0.7302\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.52977\n",
      "Epoch 87/500\n",
      "1125/1125 [==============================] - 0s 71us/step - loss: 0.4354 - acc: 0.7968 - val_loss: 0.5573 - val_acc: 0.7331\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.52977\n",
      "Epoch 88/500\n",
      "1125/1125 [==============================] - 0s 70us/step - loss: 0.4310 - acc: 0.7989 - val_loss: 0.5536 - val_acc: 0.7282\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.52977\n",
      "Epoch 89/500\n",
      "1125/1125 [==============================] - 0s 76us/step - loss: 0.4315 - acc: 0.7997 - val_loss: 0.5508 - val_acc: 0.7273\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.52977\n",
      "Epoch 90/500\n",
      "1125/1125 [==============================] - 0s 74us/step - loss: 0.4324 - acc: 0.8012 - val_loss: 0.5604 - val_acc: 0.7282\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.52977\n",
      "Epoch 91/500\n",
      "1125/1125 [==============================] - 0s 70us/step - loss: 0.4326 - acc: 0.7983 - val_loss: 0.5536 - val_acc: 0.7358\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.52977\n",
      "Epoch 92/500\n",
      "1125/1125 [==============================] - 0s 79us/step - loss: 0.4302 - acc: 0.8000 - val_loss: 0.5503 - val_acc: 0.7312\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.52977\n",
      "Epoch 93/500\n",
      "1125/1125 [==============================] - 0s 76us/step - loss: 0.4320 - acc: 0.8014 - val_loss: 0.5464 - val_acc: 0.7255\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.52977\n",
      "Epoch 94/500\n",
      "1125/1125 [==============================] - 0s 87us/step - loss: 0.4311 - acc: 0.8000 - val_loss: 0.5539 - val_acc: 0.7366\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.52977\n",
      "Epoch 95/500\n",
      "1125/1125 [==============================] - 0s 85us/step - loss: 0.4342 - acc: 0.7965 - val_loss: 0.5415 - val_acc: 0.7341\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.52977\n",
      "Epoch 96/500\n",
      "1125/1125 [==============================] - 0s 75us/step - loss: 0.4324 - acc: 0.7987 - val_loss: 0.5593 - val_acc: 0.7317\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.52977\n",
      "Epoch 97/500\n",
      "1125/1125 [==============================] - 0s 103us/step - loss: 0.4280 - acc: 0.8017 - val_loss: 0.5578 - val_acc: 0.7351\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.52977\n",
      "Epoch 98/500\n",
      "1125/1125 [==============================] - 0s 97us/step - loss: 0.4309 - acc: 0.8021 - val_loss: 0.5452 - val_acc: 0.7317\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.52977\n",
      "Epoch 99/500\n",
      "1125/1125 [==============================] - 0s 81us/step - loss: 0.4247 - acc: 0.8038 - val_loss: 0.5603 - val_acc: 0.7317\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.52977\n",
      "Epoch 100/500\n",
      "1125/1125 [==============================] - 0s 64us/step - loss: 0.4265 - acc: 0.8010 - val_loss: 0.5532 - val_acc: 0.7324\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.52977\n",
      "Epoch 101/500\n",
      "1125/1125 [==============================] - 0s 79us/step - loss: 0.4246 - acc: 0.8008 - val_loss: 0.5691 - val_acc: 0.7299\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.52977\n",
      "Epoch 102/500\n",
      "1125/1125 [==============================] - 0s 85us/step - loss: 0.4240 - acc: 0.8051 - val_loss: 0.5475 - val_acc: 0.7317\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.52977\n",
      "Epoch 103/500\n",
      "1125/1125 [==============================] - 0s 77us/step - loss: 0.4206 - acc: 0.8054 - val_loss: 0.5657 - val_acc: 0.7319\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.52977\n",
      "Epoch 104/500\n",
      "1125/1125 [==============================] - 0s 64us/step - loss: 0.4284 - acc: 0.8008 - val_loss: 0.5649 - val_acc: 0.7295\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.52977\n",
      "Epoch 105/500\n",
      "1125/1125 [==============================] - 0s 107us/step - loss: 0.4222 - acc: 0.8039 - val_loss: 0.5568 - val_acc: 0.7280\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.52977\n",
      "Epoch 106/500\n",
      "1125/1125 [==============================] - 0s 96us/step - loss: 0.4209 - acc: 0.8054 - val_loss: 0.5510 - val_acc: 0.7326\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.52977\n",
      "Epoch 107/500\n",
      "1125/1125 [==============================] - 0s 79us/step - loss: 0.4272 - acc: 0.8035 - val_loss: 0.5493 - val_acc: 0.7255\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.52977\n",
      "Epoch 108/500\n",
      "1125/1125 [==============================] - 0s 76us/step - loss: 0.4214 - acc: 0.8051 - val_loss: 0.5603 - val_acc: 0.7331\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.52977\n",
      "Epoch 109/500\n",
      "1125/1125 [==============================] - 0s 80us/step - loss: 0.4210 - acc: 0.8047 - val_loss: 0.5710 - val_acc: 0.7319\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.52977\n",
      "Epoch 110/500\n",
      "1125/1125 [==============================] - 0s 86us/step - loss: 0.4182 - acc: 0.8070 - val_loss: 0.5499 - val_acc: 0.7290\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.52977\n",
      "Epoch 111/500\n",
      "1125/1125 [==============================] - 0s 67us/step - loss: 0.4235 - acc: 0.8044 - val_loss: 0.5671 - val_acc: 0.7329\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.52977\n",
      "Epoch 112/500\n",
      "1125/1125 [==============================] - 0s 79us/step - loss: 0.4225 - acc: 0.8064 - val_loss: 0.5585 - val_acc: 0.7356\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.52977\n",
      "Epoch 113/500\n",
      "1125/1125 [==============================] - 0s 90us/step - loss: 0.4300 - acc: 0.7990 - val_loss: 0.5664 - val_acc: 0.7344\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.52977\n",
      "Epoch 114/500\n",
      "1125/1125 [==============================] - 0s 80us/step - loss: 0.4252 - acc: 0.8027 - val_loss: 0.6143 - val_acc: 0.7179\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.52977\n",
      "Epoch 115/500\n",
      "1125/1125 [==============================] - 0s 71us/step - loss: 0.4314 - acc: 0.7982 - val_loss: 0.5703 - val_acc: 0.7346\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.52977\n",
      "Epoch 116/500\n",
      "1125/1125 [==============================] - 0s 97us/step - loss: 0.4232 - acc: 0.8047 - val_loss: 0.5528 - val_acc: 0.7317\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.52977\n",
      "Epoch 117/500\n",
      "1125/1125 [==============================] - 0s 80us/step - loss: 0.4204 - acc: 0.8034 - val_loss: 0.5513 - val_acc: 0.7317\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.52977\n",
      "Epoch 118/500\n",
      "1125/1125 [==============================] - 0s 78us/step - loss: 0.4169 - acc: 0.8062 - val_loss: 0.5523 - val_acc: 0.7371\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.52977\n",
      "Epoch 119/500\n",
      "1125/1125 [==============================] - 0s 72us/step - loss: 0.4171 - acc: 0.8062 - val_loss: 0.5504 - val_acc: 0.7250\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.52977\n",
      "Epoch 120/500\n",
      "1125/1125 [==============================] - 0s 69us/step - loss: 0.4171 - acc: 0.8061 - val_loss: 0.5739 - val_acc: 0.7322\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.52977\n",
      "Epoch 121/500\n",
      "1125/1125 [==============================] - 0s 70us/step - loss: 0.4157 - acc: 0.8088 - val_loss: 0.5737 - val_acc: 0.7295\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.52977\n",
      "Epoch 122/500\n",
      "1125/1125 [==============================] - 0s 79us/step - loss: 0.4136 - acc: 0.8099 - val_loss: 0.5563 - val_acc: 0.7277\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.52977\n",
      "Epoch 123/500\n",
      "1125/1125 [==============================] - 0s 80us/step - loss: 0.4171 - acc: 0.8084 - val_loss: 0.6044 - val_acc: 0.7265\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.52977\n",
      "Epoch 124/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1125/1125 [==============================] - 0s 89us/step - loss: 0.4176 - acc: 0.8068 - val_loss: 0.6155 - val_acc: 0.7221\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.52977\n",
      "Epoch 125/500\n",
      "1125/1125 [==============================] - 0s 68us/step - loss: 0.4197 - acc: 0.8066 - val_loss: 0.5928 - val_acc: 0.7265\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.52977\n",
      "Epoch 126/500\n",
      "1125/1125 [==============================] - 0s 66us/step - loss: 0.4172 - acc: 0.8086 - val_loss: 0.5534 - val_acc: 0.7287\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.52977\n",
      "Epoch 127/500\n",
      "1125/1125 [==============================] - 0s 63us/step - loss: 0.4176 - acc: 0.8077 - val_loss: 0.5534 - val_acc: 0.7255\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.52977\n",
      "Epoch 128/500\n",
      "1125/1125 [==============================] - 0s 84us/step - loss: 0.4132 - acc: 0.8111 - val_loss: 0.5701 - val_acc: 0.7275\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.52977\n",
      "Epoch 129/500\n",
      "1125/1125 [==============================] - 0s 76us/step - loss: 0.4156 - acc: 0.8072 - val_loss: 0.5754 - val_acc: 0.7302\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.52977\n",
      "Epoch 130/500\n",
      "1125/1125 [==============================] - 0s 77us/step - loss: 0.4125 - acc: 0.8112 - val_loss: 0.5884 - val_acc: 0.7309\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.52977\n",
      "Epoch 131/500\n",
      "1125/1125 [==============================] - 0s 87us/step - loss: 0.4100 - acc: 0.8109 - val_loss: 0.6092 - val_acc: 0.7174\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.52977\n",
      "Epoch 132/500\n",
      "1125/1125 [==============================] - 0s 72us/step - loss: 0.4151 - acc: 0.8089 - val_loss: 0.5610 - val_acc: 0.7243\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.52977\n",
      "Epoch 133/500\n",
      "1125/1125 [==============================] - 0s 87us/step - loss: 0.4094 - acc: 0.8121 - val_loss: 0.5581 - val_acc: 0.7219\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.52977\n",
      "Epoch 134/500\n",
      "1125/1125 [==============================] - 0s 88us/step - loss: 0.4123 - acc: 0.8104 - val_loss: 0.5661 - val_acc: 0.7304\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.52977\n",
      "Epoch 135/500\n",
      "1125/1125 [==============================] - 0s 91us/step - loss: 0.4066 - acc: 0.8124 - val_loss: 0.5686 - val_acc: 0.7223\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.52977\n",
      "Epoch 136/500\n",
      "1125/1125 [==============================] - 0s 84us/step - loss: 0.4116 - acc: 0.8135 - val_loss: 0.5649 - val_acc: 0.7143\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.52977\n",
      "Epoch 137/500\n",
      "1125/1125 [==============================] - 0s 66us/step - loss: 0.4112 - acc: 0.8094 - val_loss: 0.5610 - val_acc: 0.7236\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.52977\n",
      "Epoch 138/500\n",
      "1125/1125 [==============================] - 0s 71us/step - loss: 0.4130 - acc: 0.8080 - val_loss: 0.5688 - val_acc: 0.7275\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.52977\n",
      "Epoch 139/500\n",
      "1125/1125 [==============================] - 0s 79us/step - loss: 0.4076 - acc: 0.8141 - val_loss: 0.5671 - val_acc: 0.7246\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.52977\n",
      "Epoch 140/500\n",
      "1125/1125 [==============================] - 0s 80us/step - loss: 0.4044 - acc: 0.8136 - val_loss: 0.5723 - val_acc: 0.7255\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.52977\n",
      "Epoch 141/500\n",
      "1125/1125 [==============================] - 0s 86us/step - loss: 0.4064 - acc: 0.8136 - val_loss: 0.5677 - val_acc: 0.7201\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.52977\n",
      "Epoch 142/500\n",
      "1125/1125 [==============================] - 0s 81us/step - loss: 0.4057 - acc: 0.8161 - val_loss: 0.5730 - val_acc: 0.7206\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.52977\n",
      "Epoch 143/500\n",
      "1125/1125 [==============================] - 0s 85us/step - loss: 0.4110 - acc: 0.8103 - val_loss: 0.5728 - val_acc: 0.7277\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.52977\n",
      "Epoch 144/500\n",
      "1125/1125 [==============================] - 0s 78us/step - loss: 0.4126 - acc: 0.8091 - val_loss: 0.5722 - val_acc: 0.7243\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.52977\n",
      "Epoch 145/500\n",
      "1125/1125 [==============================] - 0s 76us/step - loss: 0.4111 - acc: 0.8093 - val_loss: 0.5718 - val_acc: 0.7319\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.52977\n",
      "Epoch 146/500\n",
      "1125/1125 [==============================] - 0s 88us/step - loss: 0.4039 - acc: 0.8165 - val_loss: 0.5786 - val_acc: 0.7253\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.52977\n",
      "Epoch 147/500\n",
      "1125/1125 [==============================] - 0s 80us/step - loss: 0.4065 - acc: 0.8154 - val_loss: 0.5666 - val_acc: 0.7216\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.52977\n",
      "Epoch 148/500\n",
      "1125/1125 [==============================] - 0s 70us/step - loss: 0.4128 - acc: 0.8109 - val_loss: 0.5721 - val_acc: 0.7312\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.52977\n",
      "Epoch 149/500\n",
      "1125/1125 [==============================] - 0s 76us/step - loss: 0.4066 - acc: 0.8135 - val_loss: 0.5749 - val_acc: 0.7339\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.52977\n",
      "Epoch 150/500\n",
      "1125/1125 [==============================] - 0s 85us/step - loss: 0.4046 - acc: 0.8137 - val_loss: 0.5615 - val_acc: 0.7263\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.52977\n",
      "Epoch 151/500\n",
      "1125/1125 [==============================] - 0s 70us/step - loss: 0.4064 - acc: 0.8131 - val_loss: 0.5687 - val_acc: 0.7282\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.52977\n",
      "Epoch 152/500\n",
      "1125/1125 [==============================] - 0s 73us/step - loss: 0.4129 - acc: 0.8072 - val_loss: 0.5580 - val_acc: 0.7221\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.52977\n",
      "Epoch 153/500\n",
      "1125/1125 [==============================] - 0s 87us/step - loss: 0.4148 - acc: 0.8087 - val_loss: 0.5982 - val_acc: 0.7317\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.52977\n",
      "Epoch 154/500\n",
      "1125/1125 [==============================] - 0s 84us/step - loss: 0.4112 - acc: 0.8128 - val_loss: 0.5904 - val_acc: 0.7329\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.52977\n",
      "Epoch 155/500\n",
      "1125/1125 [==============================] - 0s 79us/step - loss: 0.4007 - acc: 0.8173 - val_loss: 0.5750 - val_acc: 0.7223\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.52977\n",
      "Epoch 156/500\n",
      "1125/1125 [==============================] - 0s 77us/step - loss: 0.4068 - acc: 0.8129 - val_loss: 0.5799 - val_acc: 0.7265\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.52977\n",
      "Epoch 157/500\n",
      "1125/1125 [==============================] - 0s 79us/step - loss: 0.4060 - acc: 0.8144 - val_loss: 0.6125 - val_acc: 0.7287\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.52977\n",
      "Epoch 158/500\n",
      "1125/1125 [==============================] - 0s 71us/step - loss: 0.4025 - acc: 0.8143 - val_loss: 0.5977 - val_acc: 0.7302\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.52977\n",
      "Epoch 159/500\n",
      "1125/1125 [==============================] - 0s 72us/step - loss: 0.4119 - acc: 0.8108 - val_loss: 0.5878 - val_acc: 0.7302\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.52977\n",
      "Epoch 160/500\n",
      "1125/1125 [==============================] - 0s 87us/step - loss: 0.4086 - acc: 0.8121 - val_loss: 0.5762 - val_acc: 0.7290\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.52977\n",
      "Epoch 161/500\n",
      "1125/1125 [==============================] - 0s 95us/step - loss: 0.4119 - acc: 0.8105 - val_loss: 0.5672 - val_acc: 0.7287\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.52977\n",
      "Epoch 162/500\n",
      "1125/1125 [==============================] - 0s 71us/step - loss: 0.4073 - acc: 0.8119 - val_loss: 0.5820 - val_acc: 0.7307\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.52977\n",
      "Epoch 163/500\n",
      "1125/1125 [==============================] - 0s 65us/step - loss: 0.4044 - acc: 0.8151 - val_loss: 0.5756 - val_acc: 0.7246\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.52977\n",
      "Epoch 164/500\n",
      "1125/1125 [==============================] - 0s 73us/step - loss: 0.4065 - acc: 0.8115 - val_loss: 0.5625 - val_acc: 0.7150\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.52977\n",
      "Epoch 165/500\n",
      "1125/1125 [==============================] - 0s 100us/step - loss: 0.4046 - acc: 0.8148 - val_loss: 0.5520 - val_acc: 0.7265\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.52977\n",
      "Epoch 166/500\n",
      "1125/1125 [==============================] - 0s 111us/step - loss: 0.4029 - acc: 0.8133 - val_loss: 0.5705 - val_acc: 0.7295\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.52977\n",
      "Epoch 167/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1125/1125 [==============================] - 0s 74us/step - loss: 0.3990 - acc: 0.8178 - val_loss: 0.5897 - val_acc: 0.7270\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.52977\n",
      "Epoch 168/500\n",
      "1125/1125 [==============================] - 0s 74us/step - loss: 0.3989 - acc: 0.8198 - val_loss: 0.5892 - val_acc: 0.7241\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.52977\n",
      "Epoch 169/500\n",
      "1125/1125 [==============================] - 0s 80us/step - loss: 0.4018 - acc: 0.8156 - val_loss: 0.5894 - val_acc: 0.7270\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.52977\n",
      "Epoch 170/500\n",
      "1125/1125 [==============================] - 0s 83us/step - loss: 0.3987 - acc: 0.8176 - val_loss: 0.5817 - val_acc: 0.7255\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.52977\n",
      "Epoch 171/500\n",
      "1125/1125 [==============================] - 0s 70us/step - loss: 0.3968 - acc: 0.8163 - val_loss: 0.5888 - val_acc: 0.7253\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.52977\n",
      "Epoch 172/500\n",
      "1125/1125 [==============================] - 0s 85us/step - loss: 0.3957 - acc: 0.8179 - val_loss: 0.5740 - val_acc: 0.7299\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.52977\n",
      "Epoch 173/500\n",
      "1125/1125 [==============================] - 0s 81us/step - loss: 0.3942 - acc: 0.8175 - val_loss: 0.5830 - val_acc: 0.7241\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.52977\n",
      "Epoch 174/500\n",
      "1125/1125 [==============================] - 0s 78us/step - loss: 0.3970 - acc: 0.8202 - val_loss: 0.5910 - val_acc: 0.7253\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.52977\n",
      "Epoch 175/500\n",
      "1125/1125 [==============================] - 0s 71us/step - loss: 0.3897 - acc: 0.8214 - val_loss: 0.5678 - val_acc: 0.7219\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.52977\n",
      "Epoch 176/500\n",
      "1125/1125 [==============================] - 0s 86us/step - loss: 0.3919 - acc: 0.8214 - val_loss: 0.5661 - val_acc: 0.7270\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.52977\n",
      "Epoch 177/500\n",
      "1125/1125 [==============================] - 0s 84us/step - loss: 0.3943 - acc: 0.8201 - val_loss: 0.6126 - val_acc: 0.7292\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.52977\n",
      "Epoch 178/500\n",
      "1125/1125 [==============================] - 0s 74us/step - loss: 0.3958 - acc: 0.8196 - val_loss: 0.5931 - val_acc: 0.7322\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.52977\n",
      "Epoch 179/500\n",
      "1125/1125 [==============================] - 0s 72us/step - loss: 0.3905 - acc: 0.8213 - val_loss: 0.6027 - val_acc: 0.7302\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.52977\n",
      "Epoch 180/500\n",
      "1125/1125 [==============================] - 0s 64us/step - loss: 0.3885 - acc: 0.8225 - val_loss: 0.6156 - val_acc: 0.7265\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.52977\n",
      "Epoch 181/500\n",
      "1125/1125 [==============================] - 0s 71us/step - loss: 0.3924 - acc: 0.8218 - val_loss: 0.6284 - val_acc: 0.7216\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.52977\n",
      "Epoch 182/500\n",
      "1125/1125 [==============================] - 0s 81us/step - loss: 0.3952 - acc: 0.8193 - val_loss: 0.5827 - val_acc: 0.7243\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.52977\n",
      "Epoch 183/500\n",
      "1125/1125 [==============================] - 0s 79us/step - loss: 0.3959 - acc: 0.8194 - val_loss: 0.5988 - val_acc: 0.7299\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.52977\n",
      "Epoch 184/500\n",
      "1125/1125 [==============================] - 0s 68us/step - loss: 0.3980 - acc: 0.8168 - val_loss: 0.5912 - val_acc: 0.7322\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.52977\n",
      "Epoch 185/500\n",
      "1125/1125 [==============================] - 0s 73us/step - loss: 0.3915 - acc: 0.8207 - val_loss: 0.5916 - val_acc: 0.7334\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.52977\n",
      "Epoch 186/500\n",
      "1125/1125 [==============================] - 0s 69us/step - loss: 0.3927 - acc: 0.8202 - val_loss: 0.5775 - val_acc: 0.7299\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.52977\n",
      "Epoch 187/500\n",
      "1125/1125 [==============================] - 0s 74us/step - loss: 0.3929 - acc: 0.8211 - val_loss: 0.5839 - val_acc: 0.7290\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.52977\n",
      "Epoch 188/500\n",
      "1125/1125 [==============================] - 0s 71us/step - loss: 0.3867 - acc: 0.8253 - val_loss: 0.5683 - val_acc: 0.7292\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.52977\n",
      "Epoch 189/500\n",
      "1125/1125 [==============================] - 0s 82us/step - loss: 0.3920 - acc: 0.8199 - val_loss: 0.6340 - val_acc: 0.7192\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.52977\n",
      "Epoch 190/500\n",
      "1125/1125 [==============================] - 0s 80us/step - loss: 0.3923 - acc: 0.8205 - val_loss: 0.5830 - val_acc: 0.7314\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.52977\n",
      "Epoch 191/500\n",
      "1125/1125 [==============================] - 0s 79us/step - loss: 0.3849 - acc: 0.8256 - val_loss: 0.6140 - val_acc: 0.7273\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.52977\n",
      "Epoch 192/500\n",
      "1125/1125 [==============================] - 0s 85us/step - loss: 0.3878 - acc: 0.8220 - val_loss: 0.5953 - val_acc: 0.7295\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.52977\n",
      "Epoch 193/500\n",
      "1125/1125 [==============================] - 0s 73us/step - loss: 0.3849 - acc: 0.8254 - val_loss: 0.5752 - val_acc: 0.7167\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.52977\n",
      "Epoch 194/500\n",
      "1125/1125 [==============================] - 0s 78us/step - loss: 0.3926 - acc: 0.8187 - val_loss: 0.5752 - val_acc: 0.7255\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.52977\n",
      "Epoch 195/500\n",
      "1125/1125 [==============================] - 0s 101us/step - loss: 0.3844 - acc: 0.8244 - val_loss: 0.6032 - val_acc: 0.7260\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.52977\n",
      "Epoch 196/500\n",
      "1125/1125 [==============================] - 0s 73us/step - loss: 0.3838 - acc: 0.8253 - val_loss: 0.6191 - val_acc: 0.7322\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.52977\n",
      "Epoch 197/500\n",
      "1125/1125 [==============================] - 0s 81us/step - loss: 0.3872 - acc: 0.8237 - val_loss: 0.6157 - val_acc: 0.7246\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.52977\n",
      "Epoch 198/500\n",
      "1125/1125 [==============================] - 0s 78us/step - loss: 0.3870 - acc: 0.8240 - val_loss: 0.5891 - val_acc: 0.7268\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.52977\n",
      "Epoch 199/500\n",
      "1125/1125 [==============================] - 0s 78us/step - loss: 0.3871 - acc: 0.8260 - val_loss: 0.5892 - val_acc: 0.7285\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.52977\n",
      "Epoch 200/500\n",
      "1125/1125 [==============================] - 0s 84us/step - loss: 0.3796 - acc: 0.8278 - val_loss: 0.6063 - val_acc: 0.7341\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.52977\n",
      "Epoch 201/500\n",
      "1125/1125 [==============================] - 0s 83us/step - loss: 0.3861 - acc: 0.8237 - val_loss: 0.5940 - val_acc: 0.7312\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.52977\n",
      "Epoch 202/500\n",
      "1125/1125 [==============================] - 0s 97us/step - loss: 0.3846 - acc: 0.8240 - val_loss: 0.6091 - val_acc: 0.7292\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 0.52977\n",
      "Epoch 203/500\n",
      "1125/1125 [==============================] - 0s 74us/step - loss: 0.3802 - acc: 0.8274 - val_loss: 0.6033 - val_acc: 0.7290\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 0.52977\n",
      "Epoch 204/500\n",
      "1125/1125 [==============================] - 0s 85us/step - loss: 0.3849 - acc: 0.8261 - val_loss: 0.6140 - val_acc: 0.7287\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 0.52977\n",
      "Epoch 205/500\n",
      "1125/1125 [==============================] - 0s 89us/step - loss: 0.3818 - acc: 0.8248 - val_loss: 0.6322 - val_acc: 0.7307\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 0.52977\n",
      "Epoch 206/500\n",
      "1125/1125 [==============================] - 0s 66us/step - loss: 0.3826 - acc: 0.8260 - val_loss: 0.5903 - val_acc: 0.7263\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 0.52977\n",
      "Epoch 207/500\n",
      "1125/1125 [==============================] - 0s 66us/step - loss: 0.3877 - acc: 0.8232 - val_loss: 0.6098 - val_acc: 0.7309\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 0.52977\n",
      "Epoch 208/500\n",
      "1125/1125 [==============================] - 0s 71us/step - loss: 0.3845 - acc: 0.8264 - val_loss: 0.6032 - val_acc: 0.7304\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 0.52977\n",
      "Epoch 209/500\n",
      "1125/1125 [==============================] - 0s 75us/step - loss: 0.3799 - acc: 0.8279 - val_loss: 0.6003 - val_acc: 0.7344\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 0.52977\n",
      "Epoch 210/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1125/1125 [==============================] - 0s 63us/step - loss: 0.3809 - acc: 0.8268 - val_loss: 0.5998 - val_acc: 0.7253\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 0.52977\n",
      "Epoch 211/500\n",
      "1125/1125 [==============================] - 0s 78us/step - loss: 0.3778 - acc: 0.8279 - val_loss: 0.6435 - val_acc: 0.7263\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 0.52977\n",
      "Epoch 212/500\n",
      "1125/1125 [==============================] - 0s 65us/step - loss: 0.3839 - acc: 0.8259 - val_loss: 0.5922 - val_acc: 0.7287\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 0.52977\n",
      "Epoch 213/500\n",
      "1125/1125 [==============================] - 0s 76us/step - loss: 0.3780 - acc: 0.8281 - val_loss: 0.5965 - val_acc: 0.7304\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 0.52977\n",
      "Epoch 214/500\n",
      "1125/1125 [==============================] - 0s 75us/step - loss: 0.3811 - acc: 0.8273 - val_loss: 0.5953 - val_acc: 0.7253\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 0.52977\n",
      "Epoch 215/500\n",
      "1125/1125 [==============================] - 0s 74us/step - loss: 0.3826 - acc: 0.8256 - val_loss: 0.5870 - val_acc: 0.7211\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 0.52977\n",
      "Epoch 216/500\n",
      "1125/1125 [==============================] - 0s 76us/step - loss: 0.3874 - acc: 0.8233 - val_loss: 0.5931 - val_acc: 0.7304\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 0.52977\n",
      "Epoch 217/500\n",
      "1125/1125 [==============================] - 0s 80us/step - loss: 0.3802 - acc: 0.8289 - val_loss: 0.6133 - val_acc: 0.7263\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 0.52977\n",
      "Epoch 218/500\n",
      "1125/1125 [==============================] - 0s 67us/step - loss: 0.3768 - acc: 0.8283 - val_loss: 0.6034 - val_acc: 0.7297\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 0.52977\n",
      "Epoch 219/500\n",
      "1125/1125 [==============================] - 0s 66us/step - loss: 0.3823 - acc: 0.8249 - val_loss: 0.6027 - val_acc: 0.7204\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 0.52977\n",
      "Epoch 220/500\n",
      "1125/1125 [==============================] - 0s 68us/step - loss: 0.3794 - acc: 0.8289 - val_loss: 0.5899 - val_acc: 0.7292\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 0.52977\n",
      "Epoch 221/500\n",
      "1125/1125 [==============================] - 0s 71us/step - loss: 0.3870 - acc: 0.8235 - val_loss: 0.5924 - val_acc: 0.7285\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 0.52977\n",
      "Epoch 222/500\n",
      "1125/1125 [==============================] - 0s 84us/step - loss: 0.3725 - acc: 0.8334 - val_loss: 0.5912 - val_acc: 0.7353\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 0.52977\n",
      "Epoch 223/500\n",
      "1125/1125 [==============================] - 0s 77us/step - loss: 0.3815 - acc: 0.8263 - val_loss: 0.6473 - val_acc: 0.7268\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 0.52977\n",
      "Epoch 224/500\n",
      "1125/1125 [==============================] - 0s 89us/step - loss: 0.3824 - acc: 0.8273 - val_loss: 0.6250 - val_acc: 0.7346\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 0.52977\n",
      "Epoch 225/500\n",
      "1125/1125 [==============================] - 0s 82us/step - loss: 0.3776 - acc: 0.8275 - val_loss: 0.6172 - val_acc: 0.7312\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 0.52977\n",
      "Epoch 226/500\n",
      "1125/1125 [==============================] - 0s 77us/step - loss: 0.3754 - acc: 0.8301 - val_loss: 0.5925 - val_acc: 0.7246\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 0.52977\n",
      "Epoch 227/500\n",
      "1125/1125 [==============================] - 0s 81us/step - loss: 0.3776 - acc: 0.8278 - val_loss: 0.6146 - val_acc: 0.7361\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 0.52977\n",
      "Epoch 228/500\n",
      "1125/1125 [==============================] - 0s 83us/step - loss: 0.3771 - acc: 0.8298 - val_loss: 0.6136 - val_acc: 0.7356\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 0.52977\n",
      "Epoch 229/500\n",
      "1125/1125 [==============================] - 0s 89us/step - loss: 0.3724 - acc: 0.8313 - val_loss: 0.6324 - val_acc: 0.7356\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 0.52977\n",
      "Epoch 230/500\n",
      "1125/1125 [==============================] - 0s 82us/step - loss: 0.3755 - acc: 0.8303 - val_loss: 0.6455 - val_acc: 0.7258\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 0.52977\n",
      "Epoch 231/500\n",
      "1125/1125 [==============================] - 0s 75us/step - loss: 0.3815 - acc: 0.8271 - val_loss: 0.6075 - val_acc: 0.7295\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 0.52977\n",
      "Epoch 232/500\n",
      "1125/1125 [==============================] - 0s 77us/step - loss: 0.3721 - acc: 0.8328 - val_loss: 0.5909 - val_acc: 0.7280\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 0.52977\n",
      "Epoch 233/500\n",
      "1125/1125 [==============================] - 0s 87us/step - loss: 0.3781 - acc: 0.8283 - val_loss: 0.6272 - val_acc: 0.7290\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 0.52977\n",
      "Epoch 234/500\n",
      "1125/1125 [==============================] - 0s 73us/step - loss: 0.3802 - acc: 0.8270 - val_loss: 0.5868 - val_acc: 0.7089\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 0.52977\n",
      "Epoch 235/500\n",
      "1125/1125 [==============================] - 0s 89us/step - loss: 0.3789 - acc: 0.8274 - val_loss: 0.5774 - val_acc: 0.7388\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 0.52977\n",
      "Epoch 236/500\n",
      "1125/1125 [==============================] - 0s 76us/step - loss: 0.3762 - acc: 0.8282 - val_loss: 0.5887 - val_acc: 0.7081\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 0.52977\n",
      "Epoch 237/500\n",
      "1125/1125 [==============================] - 0s 68us/step - loss: 0.3740 - acc: 0.8304 - val_loss: 0.5874 - val_acc: 0.7307\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 0.52977\n",
      "Epoch 238/500\n",
      "1125/1125 [==============================] - 0s 77us/step - loss: 0.3729 - acc: 0.8300 - val_loss: 0.6230 - val_acc: 0.7273\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 0.52977\n",
      "Epoch 239/500\n",
      "1125/1125 [==============================] - 0s 72us/step - loss: 0.3726 - acc: 0.8318 - val_loss: 0.5965 - val_acc: 0.7260\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 0.52977\n",
      "Epoch 240/500\n",
      "1125/1125 [==============================] - 0s 84us/step - loss: 0.3768 - acc: 0.8294 - val_loss: 0.6088 - val_acc: 0.7268\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 0.52977\n",
      "Epoch 241/500\n",
      "1125/1125 [==============================] - 0s 74us/step - loss: 0.3791 - acc: 0.8280 - val_loss: 0.6035 - val_acc: 0.7319\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 0.52977\n",
      "Epoch 242/500\n",
      "1125/1125 [==============================] - 0s 80us/step - loss: 0.3711 - acc: 0.8322 - val_loss: 0.5986 - val_acc: 0.7282\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 0.52977\n",
      "Epoch 243/500\n",
      "1125/1125 [==============================] - 0s 83us/step - loss: 0.3744 - acc: 0.8328 - val_loss: 0.5897 - val_acc: 0.7221\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 0.52977\n",
      "Epoch 244/500\n",
      "1125/1125 [==============================] - 0s 72us/step - loss: 0.3746 - acc: 0.8296 - val_loss: 0.6308 - val_acc: 0.7226\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 0.52977\n",
      "Epoch 245/500\n",
      "1125/1125 [==============================] - 0s 86us/step - loss: 0.3693 - acc: 0.8331 - val_loss: 0.6061 - val_acc: 0.7184\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 0.52977\n",
      "Epoch 246/500\n",
      "1125/1125 [==============================] - 0s 74us/step - loss: 0.3663 - acc: 0.8333 - val_loss: 0.5972 - val_acc: 0.7167\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 0.52977\n",
      "Epoch 247/500\n",
      "1125/1125 [==============================] - 0s 75us/step - loss: 0.3705 - acc: 0.8328 - val_loss: 0.5997 - val_acc: 0.7079\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 0.52977\n",
      "Epoch 248/500\n",
      "1125/1125 [==============================] - 0s 79us/step - loss: 0.3779 - acc: 0.8282 - val_loss: 0.5971 - val_acc: 0.7172\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 0.52977\n",
      "Epoch 249/500\n",
      "1125/1125 [==============================] - 0s 76us/step - loss: 0.3790 - acc: 0.8284 - val_loss: 0.5918 - val_acc: 0.7282\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 0.52977\n",
      "Epoch 250/500\n",
      "1125/1125 [==============================] - 0s 68us/step - loss: 0.3782 - acc: 0.8304 - val_loss: 0.5982 - val_acc: 0.7287\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 0.52977\n",
      "Epoch 251/500\n",
      "1125/1125 [==============================] - 0s 89us/step - loss: 0.3806 - acc: 0.8260 - val_loss: 0.6145 - val_acc: 0.7314\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 0.52977\n",
      "Epoch 252/500\n",
      "1125/1125 [==============================] - 0s 84us/step - loss: 0.3749 - acc: 0.8324 - val_loss: 0.6310 - val_acc: 0.7236\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 0.52977\n",
      "Epoch 253/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1125/1125 [==============================] - 0s 89us/step - loss: 0.3746 - acc: 0.8297 - val_loss: 0.6487 - val_acc: 0.7241\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 0.52977\n",
      "Epoch 254/500\n",
      "1125/1125 [==============================] - 0s 68us/step - loss: 0.3804 - acc: 0.8271 - val_loss: 0.6341 - val_acc: 0.7302\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 0.52977\n",
      "Epoch 255/500\n",
      "1125/1125 [==============================] - 0s 70us/step - loss: 0.3813 - acc: 0.8260 - val_loss: 0.5879 - val_acc: 0.7273\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 0.52977\n",
      "Epoch 256/500\n",
      "1125/1125 [==============================] - 0s 78us/step - loss: 0.3725 - acc: 0.8293 - val_loss: 0.5889 - val_acc: 0.7287\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 0.52977\n",
      "Epoch 257/500\n",
      "1125/1125 [==============================] - 0s 84us/step - loss: 0.3712 - acc: 0.8330 - val_loss: 0.6053 - val_acc: 0.7246\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 0.52977\n",
      "Epoch 258/500\n",
      "1125/1125 [==============================] - 0s 90us/step - loss: 0.3706 - acc: 0.8302 - val_loss: 0.5999 - val_acc: 0.7246\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 0.52977\n",
      "Epoch 259/500\n",
      "1125/1125 [==============================] - 0s 74us/step - loss: 0.3674 - acc: 0.8358 - val_loss: 0.6077 - val_acc: 0.7246\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 0.52977\n",
      "Epoch 260/500\n",
      "1125/1125 [==============================] - 0s 73us/step - loss: 0.3726 - acc: 0.8305 - val_loss: 0.6167 - val_acc: 0.7177\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 0.52977\n",
      "Epoch 261/500\n",
      "1125/1125 [==============================] - 0s 70us/step - loss: 0.3736 - acc: 0.8276 - val_loss: 0.6179 - val_acc: 0.7275\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 0.52977\n",
      "Epoch 262/500\n",
      "1125/1125 [==============================] - 0s 95us/step - loss: 0.3683 - acc: 0.8331 - val_loss: 0.6068 - val_acc: 0.7199\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 0.52977\n",
      "Epoch 263/500\n",
      "1125/1125 [==============================] - 0s 75us/step - loss: 0.3716 - acc: 0.8311 - val_loss: 0.6314 - val_acc: 0.7297\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 0.52977\n",
      "Epoch 264/500\n",
      "1125/1125 [==============================] - 0s 82us/step - loss: 0.3656 - acc: 0.8325 - val_loss: 0.5963 - val_acc: 0.7238\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 0.52977\n",
      "Epoch 265/500\n",
      "1125/1125 [==============================] - 0s 81us/step - loss: 0.3643 - acc: 0.8354 - val_loss: 0.6058 - val_acc: 0.7199\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 0.52977\n",
      "Epoch 266/500\n",
      "1125/1125 [==============================] - 0s 77us/step - loss: 0.3704 - acc: 0.8318 - val_loss: 0.6366 - val_acc: 0.7309\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 0.52977\n",
      "Epoch 267/500\n",
      "1125/1125 [==============================] - 0s 67us/step - loss: 0.3639 - acc: 0.8353 - val_loss: 0.6717 - val_acc: 0.7187\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 0.52977\n",
      "Epoch 268/500\n",
      "1125/1125 [==============================] - 0s 75us/step - loss: 0.3716 - acc: 0.8316 - val_loss: 0.6159 - val_acc: 0.7380\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 0.52977\n",
      "Epoch 269/500\n",
      "1125/1125 [==============================] - 0s 80us/step - loss: 0.3677 - acc: 0.8334 - val_loss: 0.5850 - val_acc: 0.7243\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 0.52977\n",
      "Epoch 270/500\n",
      "1125/1125 [==============================] - 0s 77us/step - loss: 0.3739 - acc: 0.8299 - val_loss: 0.5871 - val_acc: 0.7336\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 0.52977\n",
      "Epoch 271/500\n",
      "1125/1125 [==============================] - 0s 70us/step - loss: 0.3672 - acc: 0.8344 - val_loss: 0.6168 - val_acc: 0.7263\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 0.52977\n",
      "Epoch 272/500\n",
      "1125/1125 [==============================] - 0s 83us/step - loss: 0.3621 - acc: 0.8384 - val_loss: 0.6138 - val_acc: 0.7201\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 0.52977\n",
      "Epoch 273/500\n",
      "1125/1125 [==============================] - 0s 84us/step - loss: 0.3610 - acc: 0.8376 - val_loss: 0.6170 - val_acc: 0.7258\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 0.52977\n",
      "Epoch 274/500\n",
      "1125/1125 [==============================] - 0s 93us/step - loss: 0.3662 - acc: 0.8369 - val_loss: 0.6281 - val_acc: 0.7297\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 0.52977\n",
      "Epoch 275/500\n",
      "1125/1125 [==============================] - 0s 74us/step - loss: 0.3670 - acc: 0.8334 - val_loss: 0.6081 - val_acc: 0.7194\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 0.52977\n",
      "Epoch 276/500\n",
      "1125/1125 [==============================] - 0s 72us/step - loss: 0.3646 - acc: 0.8339 - val_loss: 0.6251 - val_acc: 0.7236\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 0.52977\n",
      "Epoch 277/500\n",
      "1125/1125 [==============================] - 0s 76us/step - loss: 0.3623 - acc: 0.8365 - val_loss: 0.6101 - val_acc: 0.7275\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 0.52977\n",
      "Epoch 278/500\n",
      "1125/1125 [==============================] - 0s 69us/step - loss: 0.3649 - acc: 0.8351 - val_loss: 0.6059 - val_acc: 0.7201\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 0.52977\n",
      "Epoch 279/500\n",
      "1125/1125 [==============================] - 0s 84us/step - loss: 0.3640 - acc: 0.8348 - val_loss: 0.6081 - val_acc: 0.7135\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 0.52977\n",
      "Epoch 280/500\n",
      "1125/1125 [==============================] - 0s 75us/step - loss: 0.3637 - acc: 0.8338 - val_loss: 0.6163 - val_acc: 0.7231\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 0.52977\n",
      "Epoch 281/500\n",
      "1125/1125 [==============================] - 0s 87us/step - loss: 0.3651 - acc: 0.8360 - val_loss: 0.6195 - val_acc: 0.7255\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 0.52977\n",
      "Epoch 282/500\n",
      "1125/1125 [==============================] - 0s 88us/step - loss: 0.3625 - acc: 0.8345 - val_loss: 0.6285 - val_acc: 0.7238\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 0.52977\n",
      "Epoch 283/500\n",
      "1125/1125 [==============================] - 0s 80us/step - loss: 0.3714 - acc: 0.8325 - val_loss: 0.5928 - val_acc: 0.7314\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 0.52977\n",
      "Epoch 284/500\n",
      "1125/1125 [==============================] - 0s 88us/step - loss: 0.3711 - acc: 0.8331 - val_loss: 0.5962 - val_acc: 0.7299\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 0.52977\n",
      "Epoch 285/500\n",
      "1125/1125 [==============================] - 0s 95us/step - loss: 0.3730 - acc: 0.8307 - val_loss: 0.6023 - val_acc: 0.7302\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 0.52977\n",
      "Epoch 286/500\n",
      "1125/1125 [==============================] - 0s 80us/step - loss: 0.3646 - acc: 0.8351 - val_loss: 0.6098 - val_acc: 0.7280\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 0.52977\n",
      "Epoch 287/500\n",
      "1125/1125 [==============================] - 0s 80us/step - loss: 0.3647 - acc: 0.8339 - val_loss: 0.6069 - val_acc: 0.7236\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 0.52977\n",
      "Epoch 288/500\n",
      "1125/1125 [==============================] - 0s 97us/step - loss: 0.3634 - acc: 0.8372 - val_loss: 0.6071 - val_acc: 0.7216\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 0.52977\n",
      "Epoch 289/500\n",
      "1125/1125 [==============================] - 0s 82us/step - loss: 0.3716 - acc: 0.8308 - val_loss: 0.5968 - val_acc: 0.7206\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 0.52977\n",
      "Epoch 290/500\n",
      "1125/1125 [==============================] - 0s 94us/step - loss: 0.3725 - acc: 0.8315 - val_loss: 0.6135 - val_acc: 0.7201\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 0.52977\n",
      "Epoch 291/500\n",
      "1125/1125 [==============================] - 0s 66us/step - loss: 0.3677 - acc: 0.8351 - val_loss: 0.6323 - val_acc: 0.7250\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 0.52977\n",
      "Epoch 292/500\n",
      "1125/1125 [==============================] - 0s 90us/step - loss: 0.3636 - acc: 0.8356 - val_loss: 0.6432 - val_acc: 0.7255\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 0.52977\n",
      "Epoch 293/500\n",
      "1125/1125 [==============================] - 0s 73us/step - loss: 0.3617 - acc: 0.8376 - val_loss: 0.6137 - val_acc: 0.7238\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 0.52977\n",
      "Epoch 294/500\n",
      "1125/1125 [==============================] - 0s 75us/step - loss: 0.3606 - acc: 0.8378 - val_loss: 0.6087 - val_acc: 0.7322\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 0.52977\n",
      "Epoch 295/500\n",
      "1125/1125 [==============================] - 0s 85us/step - loss: 0.3560 - acc: 0.8403 - val_loss: 0.6259 - val_acc: 0.7302\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 0.52977\n",
      "Epoch 296/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1125/1125 [==============================] - 0s 65us/step - loss: 0.3578 - acc: 0.8409 - val_loss: 0.6053 - val_acc: 0.7268\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 0.52977\n",
      "Epoch 297/500\n",
      "1125/1125 [==============================] - 0s 70us/step - loss: 0.3548 - acc: 0.8404 - val_loss: 0.6332 - val_acc: 0.7263\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 0.52977\n",
      "Epoch 298/500\n",
      "1125/1125 [==============================] - 0s 72us/step - loss: 0.3574 - acc: 0.8393 - val_loss: 0.6235 - val_acc: 0.7243\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 0.52977\n",
      "Epoch 299/500\n",
      "1125/1125 [==============================] - 0s 70us/step - loss: 0.3571 - acc: 0.8365 - val_loss: 0.6144 - val_acc: 0.7282\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 0.52977\n",
      "Epoch 300/500\n",
      "1125/1125 [==============================] - 0s 78us/step - loss: 0.3607 - acc: 0.8374 - val_loss: 0.6057 - val_acc: 0.7324\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 0.52977\n",
      "Epoch 301/500\n",
      "1125/1125 [==============================] - 0s 76us/step - loss: 0.3567 - acc: 0.8362 - val_loss: 0.5991 - val_acc: 0.7383\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 0.52977\n",
      "Epoch 302/500\n",
      "1125/1125 [==============================] - 0s 86us/step - loss: 0.3623 - acc: 0.8334 - val_loss: 0.6313 - val_acc: 0.7236\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 0.52977\n",
      "Epoch 303/500\n",
      "1125/1125 [==============================] - 0s 86us/step - loss: 0.3597 - acc: 0.8390 - val_loss: 0.6426 - val_acc: 0.7292\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 0.52977\n",
      "Epoch 304/500\n",
      "1125/1125 [==============================] - 0s 83us/step - loss: 0.3610 - acc: 0.8395 - val_loss: 0.6097 - val_acc: 0.7292\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 0.52977\n",
      "Epoch 305/500\n",
      "1125/1125 [==============================] - 0s 84us/step - loss: 0.3551 - acc: 0.8409 - val_loss: 0.6418 - val_acc: 0.7228\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 0.52977\n",
      "Epoch 306/500\n",
      "1125/1125 [==============================] - 0s 91us/step - loss: 0.3610 - acc: 0.8359 - val_loss: 0.6289 - val_acc: 0.7160\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 0.52977\n",
      "Epoch 307/500\n",
      "1125/1125 [==============================] - 0s 81us/step - loss: 0.3524 - acc: 0.8410 - val_loss: 0.6243 - val_acc: 0.7287\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 0.52977\n",
      "Epoch 308/500\n",
      "1125/1125 [==============================] - 0s 83us/step - loss: 0.3589 - acc: 0.8391 - val_loss: 0.5933 - val_acc: 0.7167\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 0.52977\n",
      "Epoch 309/500\n",
      "1125/1125 [==============================] - 0s 80us/step - loss: 0.3547 - acc: 0.8431 - val_loss: 0.6082 - val_acc: 0.7236\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 0.52977\n",
      "Epoch 310/500\n",
      "1125/1125 [==============================] - 0s 81us/step - loss: 0.3567 - acc: 0.8397 - val_loss: 0.6116 - val_acc: 0.7309\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 0.52977\n",
      "Epoch 311/500\n",
      "1125/1125 [==============================] - 0s 79us/step - loss: 0.3577 - acc: 0.8403 - val_loss: 0.6168 - val_acc: 0.7299\n",
      "\n",
      "Epoch 00311: val_loss did not improve from 0.52977\n",
      "Epoch 312/500\n",
      "1125/1125 [==============================] - 0s 78us/step - loss: 0.3575 - acc: 0.8391 - val_loss: 0.6132 - val_acc: 0.7285\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 0.52977\n",
      "Epoch 313/500\n",
      "1125/1125 [==============================] - 0s 71us/step - loss: 0.3573 - acc: 0.8399 - val_loss: 0.6774 - val_acc: 0.7233\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 0.52977\n",
      "Epoch 314/500\n",
      "1125/1125 [==============================] - 0s 86us/step - loss: 0.3570 - acc: 0.8399 - val_loss: 0.6004 - val_acc: 0.7236\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 0.52977\n",
      "Epoch 315/500\n",
      "1125/1125 [==============================] - 0s 83us/step - loss: 0.3674 - acc: 0.8352 - val_loss: 0.5812 - val_acc: 0.7199\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 0.52977\n",
      "Epoch 316/500\n",
      "1125/1125 [==============================] - 0s 73us/step - loss: 0.3688 - acc: 0.8326 - val_loss: 0.6033 - val_acc: 0.7368\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 0.52977\n",
      "Epoch 317/500\n",
      "1125/1125 [==============================] - 0s 93us/step - loss: 0.3611 - acc: 0.8366 - val_loss: 0.6601 - val_acc: 0.7297\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 0.52977\n",
      "Epoch 318/500\n",
      "1125/1125 [==============================] - 0s 83us/step - loss: 0.3580 - acc: 0.8363 - val_loss: 0.6343 - val_acc: 0.7341\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 0.52977\n",
      "Epoch 319/500\n",
      "1125/1125 [==============================] - 0s 83us/step - loss: 0.3567 - acc: 0.8374 - val_loss: 0.6084 - val_acc: 0.7277\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 0.52977\n",
      "Epoch 320/500\n",
      "1125/1125 [==============================] - 0s 90us/step - loss: 0.3621 - acc: 0.8381 - val_loss: 0.5955 - val_acc: 0.7317\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 0.52977\n",
      "Epoch 321/500\n",
      "1125/1125 [==============================] - 0s 78us/step - loss: 0.3654 - acc: 0.8339 - val_loss: 0.5964 - val_acc: 0.7228\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 0.52977\n",
      "Epoch 322/500\n",
      "1125/1125 [==============================] - 0s 77us/step - loss: 0.3612 - acc: 0.8367 - val_loss: 0.5933 - val_acc: 0.7250\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 0.52977\n",
      "Epoch 323/500\n",
      "1125/1125 [==============================] - 0s 77us/step - loss: 0.3653 - acc: 0.8336 - val_loss: 0.6172 - val_acc: 0.7248\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 0.52977\n",
      "Epoch 324/500\n",
      "1125/1125 [==============================] - 0s 78us/step - loss: 0.3561 - acc: 0.8413 - val_loss: 0.5918 - val_acc: 0.7238\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 0.52977\n",
      "Epoch 325/500\n",
      "1125/1125 [==============================] - 0s 80us/step - loss: 0.3612 - acc: 0.8380 - val_loss: 0.6022 - val_acc: 0.7319\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 0.52977\n",
      "Epoch 326/500\n",
      "1125/1125 [==============================] - 0s 79us/step - loss: 0.3620 - acc: 0.8370 - val_loss: 0.6074 - val_acc: 0.7263\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 0.52977\n",
      "Epoch 327/500\n",
      "1125/1125 [==============================] - 0s 85us/step - loss: 0.3562 - acc: 0.8383 - val_loss: 0.5872 - val_acc: 0.7268\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 0.52977\n",
      "Epoch 328/500\n",
      "1125/1125 [==============================] - 0s 96us/step - loss: 0.3573 - acc: 0.8394 - val_loss: 0.6217 - val_acc: 0.7287\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 0.52977\n",
      "Epoch 329/500\n",
      "1125/1125 [==============================] - 0s 83us/step - loss: 0.3588 - acc: 0.8407 - val_loss: 0.6003 - val_acc: 0.7270\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 0.52977\n",
      "Epoch 330/500\n",
      "1125/1125 [==============================] - 0s 83us/step - loss: 0.3583 - acc: 0.8381 - val_loss: 0.6457 - val_acc: 0.7268\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 0.52977\n",
      "Epoch 331/500\n",
      "1125/1125 [==============================] - 0s 88us/step - loss: 0.3626 - acc: 0.8350 - val_loss: 0.5932 - val_acc: 0.7356\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 0.52977\n",
      "Epoch 332/500\n",
      "1125/1125 [==============================] - 0s 83us/step - loss: 0.3557 - acc: 0.8396 - val_loss: 0.6187 - val_acc: 0.7292\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 0.52977\n",
      "Epoch 333/500\n",
      "1125/1125 [==============================] - 0s 75us/step - loss: 0.3567 - acc: 0.8399 - val_loss: 0.6107 - val_acc: 0.7233\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 0.52977\n",
      "Epoch 334/500\n",
      "1125/1125 [==============================] - 0s 73us/step - loss: 0.3484 - acc: 0.8447 - val_loss: 0.6288 - val_acc: 0.7329\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 0.52977\n",
      "Epoch 335/500\n",
      "1125/1125 [==============================] - 0s 72us/step - loss: 0.3460 - acc: 0.8430 - val_loss: 0.6267 - val_acc: 0.7243\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 0.52977\n",
      "Epoch 336/500\n",
      "1125/1125 [==============================] - 0s 86us/step - loss: 0.3513 - acc: 0.8402 - val_loss: 0.6201 - val_acc: 0.7221\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 0.52977\n",
      "Epoch 337/500\n",
      "1125/1125 [==============================] - 0s 75us/step - loss: 0.3579 - acc: 0.8381 - val_loss: 0.6166 - val_acc: 0.7287\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 0.52977\n",
      "Epoch 338/500\n",
      "1125/1125 [==============================] - 0s 75us/step - loss: 0.3535 - acc: 0.8406 - val_loss: 0.6103 - val_acc: 0.7309\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 0.52977\n",
      "Epoch 339/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1125/1125 [==============================] - 0s 80us/step - loss: 0.3487 - acc: 0.8446 - val_loss: 0.6354 - val_acc: 0.7299\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 0.52977\n",
      "Epoch 340/500\n",
      "1125/1125 [==============================] - 0s 75us/step - loss: 0.3535 - acc: 0.8388 - val_loss: 0.6037 - val_acc: 0.7307\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 0.52977\n",
      "Epoch 341/500\n",
      "1125/1125 [==============================] - 0s 67us/step - loss: 0.3536 - acc: 0.8385 - val_loss: 0.5971 - val_acc: 0.7241\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 0.52977\n",
      "Epoch 342/500\n",
      "1125/1125 [==============================] - 0s 74us/step - loss: 0.3525 - acc: 0.8430 - val_loss: 0.5997 - val_acc: 0.7214\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 0.52977\n",
      "Epoch 343/500\n",
      "1125/1125 [==============================] - 0s 74us/step - loss: 0.3592 - acc: 0.8361 - val_loss: 0.6068 - val_acc: 0.7253\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 0.52977\n",
      "Epoch 344/500\n",
      "1125/1125 [==============================] - 0s 72us/step - loss: 0.3564 - acc: 0.8407 - val_loss: 0.6325 - val_acc: 0.7246\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 0.52977\n",
      "Epoch 345/500\n",
      "1125/1125 [==============================] - 0s 81us/step - loss: 0.3510 - acc: 0.8414 - val_loss: 0.6272 - val_acc: 0.7326\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 0.52977\n",
      "Epoch 346/500\n",
      "1125/1125 [==============================] - 0s 73us/step - loss: 0.3481 - acc: 0.8450 - val_loss: 0.6407 - val_acc: 0.7292\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 0.52977\n",
      "Epoch 347/500\n",
      "1125/1125 [==============================] - 0s 76us/step - loss: 0.3513 - acc: 0.8412 - val_loss: 0.6346 - val_acc: 0.7248\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 0.52977\n",
      "Epoch 348/500\n",
      "1125/1125 [==============================] - 0s 98us/step - loss: 0.3504 - acc: 0.8440 - val_loss: 0.6392 - val_acc: 0.7312\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 0.52977\n",
      "Epoch 349/500\n",
      "1125/1125 [==============================] - 0s 77us/step - loss: 0.3520 - acc: 0.8439 - val_loss: 0.6414 - val_acc: 0.7243\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 0.52977\n",
      "Epoch 350/500\n",
      "1125/1125 [==============================] - 0s 84us/step - loss: 0.3473 - acc: 0.8445 - val_loss: 0.6149 - val_acc: 0.7292\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 0.52977\n",
      "Epoch 351/500\n",
      "1125/1125 [==============================] - 0s 70us/step - loss: 0.3533 - acc: 0.8439 - val_loss: 0.6060 - val_acc: 0.7351\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 0.52977\n",
      "Epoch 352/500\n",
      "1125/1125 [==============================] - 0s 74us/step - loss: 0.3512 - acc: 0.8450 - val_loss: 0.6043 - val_acc: 0.7177\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 0.52977\n",
      "Epoch 353/500\n",
      "1125/1125 [==============================] - 0s 82us/step - loss: 0.3546 - acc: 0.8391 - val_loss: 0.6468 - val_acc: 0.7277\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 0.52977\n",
      "Epoch 354/500\n",
      "1125/1125 [==============================] - 0s 81us/step - loss: 0.3523 - acc: 0.8414 - val_loss: 0.6576 - val_acc: 0.7309\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 0.52977\n",
      "Epoch 355/500\n",
      "1125/1125 [==============================] - 0s 71us/step - loss: 0.3520 - acc: 0.8411 - val_loss: 0.6252 - val_acc: 0.7353\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 0.52977\n",
      "Epoch 356/500\n",
      "1125/1125 [==============================] - 0s 73us/step - loss: 0.3521 - acc: 0.8418 - val_loss: 0.6202 - val_acc: 0.7309\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 0.52977\n",
      "Epoch 357/500\n",
      "1125/1125 [==============================] - 0s 80us/step - loss: 0.3524 - acc: 0.8393 - val_loss: 0.6230 - val_acc: 0.7277\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 0.52977\n",
      "Epoch 358/500\n",
      "1125/1125 [==============================] - 0s 66us/step - loss: 0.3559 - acc: 0.8417 - val_loss: 0.6145 - val_acc: 0.7226\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 0.52977\n",
      "Epoch 359/500\n",
      "1125/1125 [==============================] - 0s 79us/step - loss: 0.3489 - acc: 0.8444 - val_loss: 0.6571 - val_acc: 0.7287\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 0.52977\n",
      "Epoch 360/500\n",
      "1125/1125 [==============================] - 0s 81us/step - loss: 0.3474 - acc: 0.8454 - val_loss: 0.6423 - val_acc: 0.7273\n",
      "\n",
      "Epoch 00360: val_loss did not improve from 0.52977\n",
      "Epoch 361/500\n",
      "1125/1125 [==============================] - 0s 86us/step - loss: 0.3459 - acc: 0.8442 - val_loss: 0.6160 - val_acc: 0.7219\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 0.52977\n",
      "Epoch 362/500\n",
      "1125/1125 [==============================] - 0s 91us/step - loss: 0.3433 - acc: 0.8454 - val_loss: 0.6171 - val_acc: 0.7214\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 0.52977\n",
      "Epoch 363/500\n",
      "1125/1125 [==============================] - 0s 90us/step - loss: 0.3516 - acc: 0.8423 - val_loss: 0.6196 - val_acc: 0.7248\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 0.52977\n",
      "Epoch 364/500\n",
      "1125/1125 [==============================] - 0s 77us/step - loss: 0.3500 - acc: 0.8435 - val_loss: 0.6678 - val_acc: 0.7204\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 0.52977\n",
      "Epoch 365/500\n",
      "1125/1125 [==============================] - 0s 87us/step - loss: 0.3464 - acc: 0.8471 - val_loss: 0.6351 - val_acc: 0.7287\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 0.52977\n",
      "Epoch 366/500\n",
      "1125/1125 [==============================] - 0s 91us/step - loss: 0.3524 - acc: 0.8412 - val_loss: 0.6021 - val_acc: 0.7255\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 0.52977\n",
      "Epoch 367/500\n",
      "1125/1125 [==============================] - 0s 79us/step - loss: 0.3479 - acc: 0.8436 - val_loss: 0.6022 - val_acc: 0.7157\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 0.52977\n",
      "Epoch 368/500\n",
      "1125/1125 [==============================] - 0s 82us/step - loss: 0.3470 - acc: 0.8452 - val_loss: 0.6096 - val_acc: 0.7162\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 0.52977\n",
      "Epoch 369/500\n",
      "1125/1125 [==============================] - 0s 73us/step - loss: 0.3549 - acc: 0.8395 - val_loss: 0.6067 - val_acc: 0.7263\n",
      "\n",
      "Epoch 00369: val_loss did not improve from 0.52977\n",
      "Epoch 370/500\n",
      "1125/1125 [==============================] - 0s 78us/step - loss: 0.3497 - acc: 0.8445 - val_loss: 0.6325 - val_acc: 0.7263\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 0.52977\n",
      "Epoch 371/500\n",
      "1125/1125 [==============================] - 0s 91us/step - loss: 0.3437 - acc: 0.8467 - val_loss: 0.6256 - val_acc: 0.7258\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 0.52977\n",
      "Epoch 372/500\n",
      "1125/1125 [==============================] - 0s 100us/step - loss: 0.3428 - acc: 0.8450 - val_loss: 0.7218 - val_acc: 0.7177\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 0.52977\n",
      "Epoch 373/500\n",
      "1125/1125 [==============================] - 0s 86us/step - loss: 0.3493 - acc: 0.8445 - val_loss: 0.6689 - val_acc: 0.7243\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 0.52977\n",
      "Epoch 374/500\n",
      "1125/1125 [==============================] - 0s 76us/step - loss: 0.3505 - acc: 0.8435 - val_loss: 0.6371 - val_acc: 0.7228\n",
      "\n",
      "Epoch 00374: val_loss did not improve from 0.52977\n",
      "Epoch 375/500\n",
      "1125/1125 [==============================] - 0s 76us/step - loss: 0.3457 - acc: 0.8475 - val_loss: 0.6323 - val_acc: 0.7255\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 0.52977\n",
      "Epoch 376/500\n",
      "1125/1125 [==============================] - 0s 73us/step - loss: 0.3440 - acc: 0.8467 - val_loss: 0.6186 - val_acc: 0.7236\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 0.52977\n",
      "Epoch 377/500\n",
      "1125/1125 [==============================] - 0s 77us/step - loss: 0.3448 - acc: 0.8477 - val_loss: 0.6164 - val_acc: 0.7273\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 0.52977\n",
      "Epoch 378/500\n",
      "1125/1125 [==============================] - 0s 73us/step - loss: 0.3437 - acc: 0.8466 - val_loss: 0.6293 - val_acc: 0.7209\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 0.52977\n",
      "Epoch 379/500\n",
      "1125/1125 [==============================] - 0s 75us/step - loss: 0.3375 - acc: 0.8522 - val_loss: 0.6251 - val_acc: 0.7172\n",
      "\n",
      "Epoch 00379: val_loss did not improve from 0.52977\n",
      "Epoch 380/500\n",
      "1125/1125 [==============================] - 0s 67us/step - loss: 0.3425 - acc: 0.8483 - val_loss: 0.6326 - val_acc: 0.7299\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 0.52977\n",
      "Epoch 381/500\n",
      "1125/1125 [==============================] - 0s 70us/step - loss: 0.3426 - acc: 0.8470 - val_loss: 0.6347 - val_acc: 0.7295\n",
      "\n",
      "Epoch 00381: val_loss did not improve from 0.52977\n",
      "Epoch 382/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1125/1125 [==============================] - 0s 62us/step - loss: 0.3367 - acc: 0.8500 - val_loss: 0.6133 - val_acc: 0.7246\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 0.52977\n",
      "Epoch 383/500\n",
      "1125/1125 [==============================] - 0s 76us/step - loss: 0.3426 - acc: 0.8463 - val_loss: 0.5993 - val_acc: 0.7331\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 0.52977\n",
      "Epoch 384/500\n",
      "1125/1125 [==============================] - 0s 91us/step - loss: 0.3450 - acc: 0.8457 - val_loss: 0.6440 - val_acc: 0.7297\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 0.52977\n",
      "Epoch 385/500\n",
      "1125/1125 [==============================] - 0s 76us/step - loss: 0.3432 - acc: 0.8452 - val_loss: 0.6836 - val_acc: 0.7265\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 0.52977\n",
      "Epoch 386/500\n",
      "1125/1125 [==============================] - 0s 84us/step - loss: 0.3458 - acc: 0.8450 - val_loss: 0.6547 - val_acc: 0.7265\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 0.52977\n",
      "Epoch 387/500\n",
      "1125/1125 [==============================] - 0s 77us/step - loss: 0.3480 - acc: 0.8461 - val_loss: 0.6197 - val_acc: 0.7346\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 0.52977\n",
      "Epoch 388/500\n",
      "1125/1125 [==============================] - 0s 72us/step - loss: 0.3446 - acc: 0.8436 - val_loss: 0.6240 - val_acc: 0.7275\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 0.52977\n",
      "Epoch 389/500\n",
      "1125/1125 [==============================] - 0s 92us/step - loss: 0.3443 - acc: 0.8464 - val_loss: 0.6296 - val_acc: 0.7196\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 0.52977\n",
      "Epoch 390/500\n",
      "1125/1125 [==============================] - 0s 81us/step - loss: 0.3463 - acc: 0.8448 - val_loss: 0.7153 - val_acc: 0.7160\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 0.52977\n",
      "Epoch 391/500\n",
      "1125/1125 [==============================] - 0s 81us/step - loss: 0.3522 - acc: 0.8441 - val_loss: 0.6203 - val_acc: 0.7211\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 0.52977\n",
      "Epoch 392/500\n",
      "1125/1125 [==============================] - 0s 78us/step - loss: 0.3485 - acc: 0.8435 - val_loss: 0.6469 - val_acc: 0.7263\n",
      "\n",
      "Epoch 00392: val_loss did not improve from 0.52977\n",
      "Epoch 393/500\n",
      "1125/1125 [==============================] - 0s 73us/step - loss: 0.3482 - acc: 0.8430 - val_loss: 0.6180 - val_acc: 0.7273\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 0.52977\n",
      "Epoch 394/500\n",
      "1125/1125 [==============================] - 0s 86us/step - loss: 0.3439 - acc: 0.8452 - val_loss: 0.6376 - val_acc: 0.7312\n",
      "\n",
      "Epoch 00394: val_loss did not improve from 0.52977\n",
      "Epoch 395/500\n",
      "1125/1125 [==============================] - 0s 83us/step - loss: 0.3423 - acc: 0.8453 - val_loss: 0.6151 - val_acc: 0.7182\n",
      "\n",
      "Epoch 00395: val_loss did not improve from 0.52977\n",
      "Epoch 396/500\n",
      "1125/1125 [==============================] - 0s 99us/step - loss: 0.3436 - acc: 0.8463 - val_loss: 0.6174 - val_acc: 0.7209\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 0.52977\n",
      "Epoch 397/500\n",
      "1125/1125 [==============================] - 0s 92us/step - loss: 0.3506 - acc: 0.8417 - val_loss: 0.6140 - val_acc: 0.7302\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 0.52977\n",
      "Epoch 398/500\n",
      "1125/1125 [==============================] - 0s 83us/step - loss: 0.3453 - acc: 0.8442 - val_loss: 0.6229 - val_acc: 0.7194\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 0.52977\n",
      "Epoch 399/500\n",
      "1125/1125 [==============================] - 0s 72us/step - loss: 0.3400 - acc: 0.8465 - val_loss: 0.6624 - val_acc: 0.7250\n",
      "\n",
      "Epoch 00399: val_loss did not improve from 0.52977\n",
      "Epoch 400/500\n",
      "1125/1125 [==============================] - 0s 84us/step - loss: 0.3424 - acc: 0.8478 - val_loss: 0.6205 - val_acc: 0.7268\n",
      "\n",
      "Epoch 00400: val_loss did not improve from 0.52977\n",
      "Epoch 401/500\n",
      "1125/1125 [==============================] - 0s 80us/step - loss: 0.3445 - acc: 0.8464 - val_loss: 0.6164 - val_acc: 0.7287\n",
      "\n",
      "Epoch 00401: val_loss did not improve from 0.52977\n",
      "Epoch 402/500\n",
      "1125/1125 [==============================] - 0s 69us/step - loss: 0.3384 - acc: 0.8484 - val_loss: 0.7002 - val_acc: 0.7236\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 0.52977\n",
      "Epoch 403/500\n",
      "1125/1125 [==============================] - 0s 86us/step - loss: 0.3430 - acc: 0.8454 - val_loss: 0.6520 - val_acc: 0.7302\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 0.52977\n",
      "Epoch 404/500\n",
      "1125/1125 [==============================] - 0s 83us/step - loss: 0.3398 - acc: 0.8495 - val_loss: 0.6284 - val_acc: 0.7211\n",
      "\n",
      "Epoch 00404: val_loss did not improve from 0.52977\n",
      "Epoch 405/500\n",
      "1125/1125 [==============================] - 0s 80us/step - loss: 0.3466 - acc: 0.8459 - val_loss: 0.6384 - val_acc: 0.7219\n",
      "\n",
      "Epoch 00405: val_loss did not improve from 0.52977\n",
      "Epoch 406/500\n",
      "1125/1125 [==============================] - 0s 80us/step - loss: 0.3422 - acc: 0.8461 - val_loss: 0.6304 - val_acc: 0.7162\n",
      "\n",
      "Epoch 00406: val_loss did not improve from 0.52977\n",
      "Epoch 407/500\n",
      "1125/1125 [==============================] - 0s 81us/step - loss: 0.3401 - acc: 0.8486 - val_loss: 0.6474 - val_acc: 0.7150\n",
      "\n",
      "Epoch 00407: val_loss did not improve from 0.52977\n",
      "Epoch 408/500\n",
      "1125/1125 [==============================] - 0s 79us/step - loss: 0.3390 - acc: 0.8479 - val_loss: 0.6255 - val_acc: 0.7228\n",
      "\n",
      "Epoch 00408: val_loss did not improve from 0.52977\n",
      "Epoch 409/500\n",
      "1125/1125 [==============================] - 0s 77us/step - loss: 0.3459 - acc: 0.8432 - val_loss: 0.6293 - val_acc: 0.7150\n",
      "\n",
      "Epoch 00409: val_loss did not improve from 0.52977\n",
      "Epoch 410/500\n",
      "1125/1125 [==============================] - 0s 81us/step - loss: 0.3410 - acc: 0.8494 - val_loss: 0.6195 - val_acc: 0.7172\n",
      "\n",
      "Epoch 00410: val_loss did not improve from 0.52977\n",
      "Epoch 411/500\n",
      "1125/1125 [==============================] - 0s 77us/step - loss: 0.3414 - acc: 0.8468 - val_loss: 0.6326 - val_acc: 0.7182\n",
      "\n",
      "Epoch 00411: val_loss did not improve from 0.52977\n",
      "Epoch 412/500\n",
      "1125/1125 [==============================] - 0s 71us/step - loss: 0.3440 - acc: 0.8473 - val_loss: 0.6550 - val_acc: 0.7228\n",
      "\n",
      "Epoch 00412: val_loss did not improve from 0.52977\n",
      "Epoch 413/500\n",
      "1125/1125 [==============================] - 0s 81us/step - loss: 0.3366 - acc: 0.8509 - val_loss: 0.6500 - val_acc: 0.7179\n",
      "\n",
      "Epoch 00413: val_loss did not improve from 0.52977\n",
      "Epoch 414/500\n",
      "1125/1125 [==============================] - 0s 72us/step - loss: 0.3377 - acc: 0.8479 - val_loss: 0.6722 - val_acc: 0.7204\n",
      "\n",
      "Epoch 00414: val_loss did not improve from 0.52977\n",
      "Epoch 415/500\n",
      "1125/1125 [==============================] - 0s 78us/step - loss: 0.3449 - acc: 0.8461 - val_loss: 0.6302 - val_acc: 0.7226\n",
      "\n",
      "Epoch 00415: val_loss did not improve from 0.52977\n",
      "Epoch 416/500\n",
      "1125/1125 [==============================] - 0s 84us/step - loss: 0.3433 - acc: 0.8467 - val_loss: 0.6418 - val_acc: 0.7292\n",
      "\n",
      "Epoch 00416: val_loss did not improve from 0.52977\n",
      "Epoch 417/500\n",
      "1125/1125 [==============================] - 0s 96us/step - loss: 0.3438 - acc: 0.8468 - val_loss: 0.6725 - val_acc: 0.7265\n",
      "\n",
      "Epoch 00417: val_loss did not improve from 0.52977\n",
      "Epoch 418/500\n",
      "1125/1125 [==============================] - 0s 79us/step - loss: 0.3395 - acc: 0.8480 - val_loss: 0.6318 - val_acc: 0.7243\n",
      "\n",
      "Epoch 00418: val_loss did not improve from 0.52977\n",
      "Epoch 419/500\n",
      "1125/1125 [==============================] - 0s 79us/step - loss: 0.3421 - acc: 0.8467 - val_loss: 0.6257 - val_acc: 0.7140\n",
      "\n",
      "Epoch 00419: val_loss did not improve from 0.52977\n",
      "Epoch 420/500\n",
      "1125/1125 [==============================] - 0s 81us/step - loss: 0.3455 - acc: 0.8449 - val_loss: 0.6248 - val_acc: 0.7246\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 0.52977\n",
      "Epoch 421/500\n",
      "1125/1125 [==============================] - 0s 81us/step - loss: 0.3370 - acc: 0.8502 - val_loss: 0.6153 - val_acc: 0.7167\n",
      "\n",
      "Epoch 00421: val_loss did not improve from 0.52977\n",
      "Epoch 422/500\n",
      "1125/1125 [==============================] - 0s 77us/step - loss: 0.3392 - acc: 0.8481 - val_loss: 0.6366 - val_acc: 0.7280\n",
      "\n",
      "Epoch 00422: val_loss did not improve from 0.52977\n",
      "Epoch 423/500\n",
      "1125/1125 [==============================] - 0s 83us/step - loss: 0.3366 - acc: 0.8486 - val_loss: 0.6328 - val_acc: 0.7302\n",
      "\n",
      "Epoch 00423: val_loss did not improve from 0.52977\n",
      "Epoch 424/500\n",
      "1125/1125 [==============================] - 0s 69us/step - loss: 0.3420 - acc: 0.8465 - val_loss: 0.6302 - val_acc: 0.7236\n",
      "\n",
      "Epoch 00424: val_loss did not improve from 0.52977\n",
      "Epoch 425/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1125/1125 [==============================] - 0s 82us/step - loss: 0.3396 - acc: 0.8464 - val_loss: 0.6389 - val_acc: 0.7268\n",
      "\n",
      "Epoch 00425: val_loss did not improve from 0.52977\n",
      "Epoch 426/500\n",
      "1125/1125 [==============================] - 0s 83us/step - loss: 0.3399 - acc: 0.8470 - val_loss: 0.6210 - val_acc: 0.7248\n",
      "\n",
      "Epoch 00426: val_loss did not improve from 0.52977\n",
      "Epoch 427/500\n",
      "1125/1125 [==============================] - 0s 85us/step - loss: 0.3360 - acc: 0.8491 - val_loss: 0.6444 - val_acc: 0.7108\n",
      "\n",
      "Epoch 00427: val_loss did not improve from 0.52977\n",
      "Epoch 428/500\n",
      "1125/1125 [==============================] - 0s 77us/step - loss: 0.3365 - acc: 0.8517 - val_loss: 0.6463 - val_acc: 0.7275\n",
      "\n",
      "Epoch 00428: val_loss did not improve from 0.52977\n",
      "Epoch 429/500\n",
      "1125/1125 [==============================] - 0s 82us/step - loss: 0.3360 - acc: 0.8510 - val_loss: 0.6343 - val_acc: 0.7277\n",
      "\n",
      "Epoch 00429: val_loss did not improve from 0.52977\n",
      "Epoch 430/500\n",
      "1125/1125 [==============================] - 0s 76us/step - loss: 0.3366 - acc: 0.8487 - val_loss: 0.6225 - val_acc: 0.7182\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 0.52977\n",
      "Epoch 431/500\n",
      "1125/1125 [==============================] - 0s 82us/step - loss: 0.3444 - acc: 0.8443 - val_loss: 0.6208 - val_acc: 0.7253\n",
      "\n",
      "Epoch 00431: val_loss did not improve from 0.52977\n",
      "Epoch 432/500\n",
      "1125/1125 [==============================] - 0s 87us/step - loss: 0.3321 - acc: 0.8511 - val_loss: 0.6571 - val_acc: 0.7196\n",
      "\n",
      "Epoch 00432: val_loss did not improve from 0.52977\n",
      "Epoch 433/500\n",
      "1125/1125 [==============================] - 0s 76us/step - loss: 0.3354 - acc: 0.8505 - val_loss: 0.6467 - val_acc: 0.7243\n",
      "\n",
      "Epoch 00433: val_loss did not improve from 0.52977\n",
      "Epoch 434/500\n",
      "1125/1125 [==============================] - 0s 72us/step - loss: 0.3354 - acc: 0.8484 - val_loss: 0.6694 - val_acc: 0.7214\n",
      "\n",
      "Epoch 00434: val_loss did not improve from 0.52977\n",
      "Epoch 435/500\n",
      "1125/1125 [==============================] - 0s 77us/step - loss: 0.3395 - acc: 0.8474 - val_loss: 0.6452 - val_acc: 0.7258\n",
      "\n",
      "Epoch 00435: val_loss did not improve from 0.52977\n",
      "Epoch 436/500\n",
      "1125/1125 [==============================] - 0s 77us/step - loss: 0.3390 - acc: 0.8487 - val_loss: 0.6473 - val_acc: 0.7243\n",
      "\n",
      "Epoch 00436: val_loss did not improve from 0.52977\n",
      "Epoch 437/500\n",
      "1125/1125 [==============================] - 0s 78us/step - loss: 0.3399 - acc: 0.8476 - val_loss: 0.6428 - val_acc: 0.7241\n",
      "\n",
      "Epoch 00437: val_loss did not improve from 0.52977\n",
      "Epoch 438/500\n",
      "1125/1125 [==============================] - 0s 75us/step - loss: 0.3349 - acc: 0.8522 - val_loss: 0.6664 - val_acc: 0.7287\n",
      "\n",
      "Epoch 00438: val_loss did not improve from 0.52977\n",
      "Epoch 439/500\n",
      "1125/1125 [==============================] - 0s 85us/step - loss: 0.3384 - acc: 0.8500 - val_loss: 0.6453 - val_acc: 0.7216\n",
      "\n",
      "Epoch 00439: val_loss did not improve from 0.52977\n",
      "Epoch 440/500\n",
      "1125/1125 [==============================] - 0s 84us/step - loss: 0.3384 - acc: 0.8457 - val_loss: 0.6760 - val_acc: 0.7243\n",
      "\n",
      "Epoch 00440: val_loss did not improve from 0.52977\n",
      "Epoch 441/500\n",
      "1125/1125 [==============================] - 0s 78us/step - loss: 0.3393 - acc: 0.8479 - val_loss: 0.6401 - val_acc: 0.7189\n",
      "\n",
      "Epoch 00441: val_loss did not improve from 0.52977\n",
      "Epoch 442/500\n",
      "1125/1125 [==============================] - 0s 80us/step - loss: 0.3364 - acc: 0.8519 - val_loss: 0.6604 - val_acc: 0.7238\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 0.52977\n",
      "Epoch 443/500\n",
      "1125/1125 [==============================] - 0s 88us/step - loss: 0.3366 - acc: 0.8500 - val_loss: 0.6192 - val_acc: 0.7265\n",
      "\n",
      "Epoch 00443: val_loss did not improve from 0.52977\n",
      "Epoch 444/500\n",
      "1125/1125 [==============================] - 0s 83us/step - loss: 0.3389 - acc: 0.8488 - val_loss: 0.6241 - val_acc: 0.7260\n",
      "\n",
      "Epoch 00444: val_loss did not improve from 0.52977\n",
      "Epoch 445/500\n",
      "1125/1125 [==============================] - 0s 91us/step - loss: 0.3380 - acc: 0.8485 - val_loss: 0.6370 - val_acc: 0.7314\n",
      "\n",
      "Epoch 00445: val_loss did not improve from 0.52977\n",
      "Epoch 446/500\n",
      "1125/1125 [==============================] - 0s 78us/step - loss: 0.3387 - acc: 0.8488 - val_loss: 0.6269 - val_acc: 0.7304\n",
      "\n",
      "Epoch 00446: val_loss did not improve from 0.52977\n",
      "Epoch 447/500\n",
      "1125/1125 [==============================] - 0s 71us/step - loss: 0.3389 - acc: 0.8482 - val_loss: 0.6889 - val_acc: 0.7309\n",
      "\n",
      "Epoch 00447: val_loss did not improve from 0.52977\n",
      "Epoch 448/500\n",
      "1125/1125 [==============================] - 0s 83us/step - loss: 0.3329 - acc: 0.8526 - val_loss: 0.6323 - val_acc: 0.7292\n",
      "\n",
      "Epoch 00448: val_loss did not improve from 0.52977\n",
      "Epoch 449/500\n",
      "1125/1125 [==============================] - 0s 91us/step - loss: 0.3347 - acc: 0.8520 - val_loss: 0.6496 - val_acc: 0.7314\n",
      "\n",
      "Epoch 00449: val_loss did not improve from 0.52977\n",
      "Epoch 450/500\n",
      "1125/1125 [==============================] - 0s 74us/step - loss: 0.3377 - acc: 0.8488 - val_loss: 0.6940 - val_acc: 0.7304\n",
      "\n",
      "Epoch 00450: val_loss did not improve from 0.52977\n",
      "Epoch 451/500\n",
      "1125/1125 [==============================] - 0s 80us/step - loss: 0.3329 - acc: 0.8504 - val_loss: 0.6377 - val_acc: 0.7299\n",
      "\n",
      "Epoch 00451: val_loss did not improve from 0.52977\n",
      "Epoch 452/500\n",
      "1125/1125 [==============================] - 0s 102us/step - loss: 0.3383 - acc: 0.8487 - val_loss: 0.6564 - val_acc: 0.7226\n",
      "\n",
      "Epoch 00452: val_loss did not improve from 0.52977\n",
      "Epoch 453/500\n",
      "1125/1125 [==============================] - 0s 62us/step - loss: 0.3385 - acc: 0.8482 - val_loss: 0.6376 - val_acc: 0.7260\n",
      "\n",
      "Epoch 00453: val_loss did not improve from 0.52977\n",
      "Epoch 454/500\n",
      "1125/1125 [==============================] - 0s 69us/step - loss: 0.3376 - acc: 0.8509 - val_loss: 0.6760 - val_acc: 0.7280\n",
      "\n",
      "Epoch 00454: val_loss did not improve from 0.52977\n",
      "Epoch 455/500\n",
      "1125/1125 [==============================] - 0s 75us/step - loss: 0.3369 - acc: 0.8509 - val_loss: 0.6837 - val_acc: 0.7314\n",
      "\n",
      "Epoch 00455: val_loss did not improve from 0.52977\n",
      "Epoch 456/500\n",
      "1125/1125 [==============================] - 0s 85us/step - loss: 0.3404 - acc: 0.8488 - val_loss: 0.6739 - val_acc: 0.7236\n",
      "\n",
      "Epoch 00456: val_loss did not improve from 0.52977\n",
      "Epoch 457/500\n",
      "1125/1125 [==============================] - 0s 91us/step - loss: 0.3336 - acc: 0.8514 - val_loss: 0.6357 - val_acc: 0.7236\n",
      "\n",
      "Epoch 00457: val_loss did not improve from 0.52977\n",
      "Epoch 458/500\n",
      "1125/1125 [==============================] - 0s 73us/step - loss: 0.3330 - acc: 0.8520 - val_loss: 0.6628 - val_acc: 0.7233\n",
      "\n",
      "Epoch 00458: val_loss did not improve from 0.52977\n",
      "Epoch 459/500\n",
      "1125/1125 [==============================] - 0s 99us/step - loss: 0.3319 - acc: 0.8539 - val_loss: 0.6885 - val_acc: 0.7246\n",
      "\n",
      "Epoch 00459: val_loss did not improve from 0.52977\n",
      "Epoch 460/500\n",
      "1125/1125 [==============================] - 0s 88us/step - loss: 0.3289 - acc: 0.8542 - val_loss: 0.6394 - val_acc: 0.7273\n",
      "\n",
      "Epoch 00460: val_loss did not improve from 0.52977\n",
      "Epoch 461/500\n",
      "1125/1125 [==============================] - 0s 82us/step - loss: 0.3352 - acc: 0.8503 - val_loss: 0.6278 - val_acc: 0.7263\n",
      "\n",
      "Epoch 00461: val_loss did not improve from 0.52977\n",
      "Epoch 462/500\n",
      "1125/1125 [==============================] - 0s 71us/step - loss: 0.3304 - acc: 0.8533 - val_loss: 0.6498 - val_acc: 0.7221\n",
      "\n",
      "Epoch 00462: val_loss did not improve from 0.52977\n",
      "Epoch 463/500\n",
      "1125/1125 [==============================] - 0s 68us/step - loss: 0.3357 - acc: 0.8512 - val_loss: 0.6484 - val_acc: 0.7253\n",
      "\n",
      "Epoch 00463: val_loss did not improve from 0.52977\n",
      "Epoch 464/500\n",
      "1125/1125 [==============================] - 0s 78us/step - loss: 0.3349 - acc: 0.8501 - val_loss: 0.6662 - val_acc: 0.7236\n",
      "\n",
      "Epoch 00464: val_loss did not improve from 0.52977\n",
      "Epoch 465/500\n",
      "1125/1125 [==============================] - 0s 83us/step - loss: 0.3361 - acc: 0.8479 - val_loss: 0.6689 - val_acc: 0.7253\n",
      "\n",
      "Epoch 00465: val_loss did not improve from 0.52977\n",
      "Epoch 466/500\n",
      "1125/1125 [==============================] - 0s 73us/step - loss: 0.3388 - acc: 0.8480 - val_loss: 0.6225 - val_acc: 0.7292\n",
      "\n",
      "Epoch 00466: val_loss did not improve from 0.52977\n",
      "Epoch 467/500\n",
      "1125/1125 [==============================] - 0s 83us/step - loss: 0.3325 - acc: 0.8520 - val_loss: 0.6301 - val_acc: 0.7307\n",
      "\n",
      "Epoch 00467: val_loss did not improve from 0.52977\n",
      "Epoch 468/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1125/1125 [==============================] - 0s 85us/step - loss: 0.3295 - acc: 0.8559 - val_loss: 0.6466 - val_acc: 0.7324\n",
      "\n",
      "Epoch 00468: val_loss did not improve from 0.52977\n",
      "Epoch 469/500\n",
      "1125/1125 [==============================] - 0s 67us/step - loss: 0.3337 - acc: 0.8537 - val_loss: 0.6719 - val_acc: 0.7216\n",
      "\n",
      "Epoch 00469: val_loss did not improve from 0.52977\n",
      "Epoch 470/500\n",
      "1125/1125 [==============================] - 0s 81us/step - loss: 0.3408 - acc: 0.8466 - val_loss: 0.6777 - val_acc: 0.7319\n",
      "\n",
      "Epoch 00470: val_loss did not improve from 0.52977\n",
      "Epoch 471/500\n",
      "1125/1125 [==============================] - 0s 81us/step - loss: 0.3365 - acc: 0.8491 - val_loss: 0.6351 - val_acc: 0.7331\n",
      "\n",
      "Epoch 00471: val_loss did not improve from 0.52977\n",
      "Epoch 472/500\n",
      "1125/1125 [==============================] - 0s 73us/step - loss: 0.3375 - acc: 0.8479 - val_loss: 0.6445 - val_acc: 0.7312\n",
      "\n",
      "Epoch 00472: val_loss did not improve from 0.52977\n",
      "Epoch 473/500\n",
      "1125/1125 [==============================] - 0s 79us/step - loss: 0.3400 - acc: 0.8480 - val_loss: 0.6250 - val_acc: 0.7248\n",
      "\n",
      "Epoch 00473: val_loss did not improve from 0.52977\n",
      "Epoch 474/500\n",
      "1125/1125 [==============================] - 0s 69us/step - loss: 0.3419 - acc: 0.8481 - val_loss: 0.6198 - val_acc: 0.7201\n",
      "\n",
      "Epoch 00474: val_loss did not improve from 0.52977\n",
      "Epoch 475/500\n",
      "1125/1125 [==============================] - 0s 80us/step - loss: 0.3427 - acc: 0.8482 - val_loss: 0.6216 - val_acc: 0.7322\n",
      "\n",
      "Epoch 00475: val_loss did not improve from 0.52977\n",
      "Epoch 476/500\n",
      "1125/1125 [==============================] - 0s 75us/step - loss: 0.3374 - acc: 0.8498 - val_loss: 0.6710 - val_acc: 0.7263\n",
      "\n",
      "Epoch 00476: val_loss did not improve from 0.52977\n",
      "Epoch 477/500\n",
      "1125/1125 [==============================] - 0s 79us/step - loss: 0.3315 - acc: 0.8519 - val_loss: 0.6624 - val_acc: 0.7236\n",
      "\n",
      "Epoch 00477: val_loss did not improve from 0.52977\n",
      "Epoch 478/500\n",
      "1125/1125 [==============================] - 0s 80us/step - loss: 0.3317 - acc: 0.8527 - val_loss: 0.6291 - val_acc: 0.7270\n",
      "\n",
      "Epoch 00478: val_loss did not improve from 0.52977\n",
      "Epoch 479/500\n",
      "1125/1125 [==============================] - 0s 89us/step - loss: 0.3282 - acc: 0.8550 - val_loss: 0.6253 - val_acc: 0.7231\n",
      "\n",
      "Epoch 00479: val_loss did not improve from 0.52977\n",
      "Epoch 480/500\n",
      "1125/1125 [==============================] - 0s 74us/step - loss: 0.3342 - acc: 0.8518 - val_loss: 0.6619 - val_acc: 0.7214\n",
      "\n",
      "Epoch 00480: val_loss did not improve from 0.52977\n",
      "Epoch 481/500\n",
      "1125/1125 [==============================] - 0s 95us/step - loss: 0.3358 - acc: 0.8520 - val_loss: 0.6246 - val_acc: 0.7265\n",
      "\n",
      "Epoch 00481: val_loss did not improve from 0.52977\n",
      "Epoch 482/500\n",
      "1125/1125 [==============================] - 0s 94us/step - loss: 0.3262 - acc: 0.8554 - val_loss: 0.6360 - val_acc: 0.7165\n",
      "\n",
      "Epoch 00482: val_loss did not improve from 0.52977\n",
      "Epoch 483/500\n",
      "1125/1125 [==============================] - 0s 70us/step - loss: 0.3343 - acc: 0.8502 - val_loss: 0.6583 - val_acc: 0.7277\n",
      "\n",
      "Epoch 00483: val_loss did not improve from 0.52977\n",
      "Epoch 484/500\n",
      "1125/1125 [==============================] - 0s 74us/step - loss: 0.3301 - acc: 0.8552 - val_loss: 0.6797 - val_acc: 0.7255\n",
      "\n",
      "Epoch 00484: val_loss did not improve from 0.52977\n",
      "Epoch 485/500\n",
      "1125/1125 [==============================] - 0s 74us/step - loss: 0.3324 - acc: 0.8524 - val_loss: 0.6667 - val_acc: 0.7236\n",
      "\n",
      "Epoch 00485: val_loss did not improve from 0.52977\n",
      "Epoch 486/500\n",
      "1125/1125 [==============================] - 0s 83us/step - loss: 0.3381 - acc: 0.8494 - val_loss: 0.6350 - val_acc: 0.7209\n",
      "\n",
      "Epoch 00486: val_loss did not improve from 0.52977\n",
      "Epoch 487/500\n",
      "1125/1125 [==============================] - 0s 70us/step - loss: 0.3285 - acc: 0.8534 - val_loss: 0.6338 - val_acc: 0.7312\n",
      "\n",
      "Epoch 00487: val_loss did not improve from 0.52977\n",
      "Epoch 488/500\n",
      "1125/1125 [==============================] - 0s 90us/step - loss: 0.3299 - acc: 0.8534 - val_loss: 0.6271 - val_acc: 0.7238\n",
      "\n",
      "Epoch 00488: val_loss did not improve from 0.52977\n",
      "Epoch 489/500\n",
      "1125/1125 [==============================] - 0s 63us/step - loss: 0.3342 - acc: 0.8518 - val_loss: 0.6155 - val_acc: 0.7277\n",
      "\n",
      "Epoch 00489: val_loss did not improve from 0.52977\n",
      "Epoch 490/500\n",
      "1125/1125 [==============================] - 0s 73us/step - loss: 0.3279 - acc: 0.8539 - val_loss: 0.6333 - val_acc: 0.7265\n",
      "\n",
      "Epoch 00490: val_loss did not improve from 0.52977\n",
      "Epoch 491/500\n",
      "1125/1125 [==============================] - 0s 76us/step - loss: 0.3321 - acc: 0.8515 - val_loss: 0.6383 - val_acc: 0.7265\n",
      "\n",
      "Epoch 00491: val_loss did not improve from 0.52977\n",
      "Epoch 492/500\n",
      "1125/1125 [==============================] - 0s 72us/step - loss: 0.3277 - acc: 0.8552 - val_loss: 0.6429 - val_acc: 0.7214\n",
      "\n",
      "Epoch 00492: val_loss did not improve from 0.52977\n",
      "Epoch 493/500\n",
      "1125/1125 [==============================] - 0s 84us/step - loss: 0.3279 - acc: 0.8527 - val_loss: 0.6683 - val_acc: 0.7319\n",
      "\n",
      "Epoch 00493: val_loss did not improve from 0.52977\n",
      "Epoch 494/500\n",
      "1125/1125 [==============================] - 0s 88us/step - loss: 0.3279 - acc: 0.8529 - val_loss: 0.6627 - val_acc: 0.7273\n",
      "\n",
      "Epoch 00494: val_loss did not improve from 0.52977\n",
      "Epoch 495/500\n",
      "1125/1125 [==============================] - 0s 76us/step - loss: 0.3306 - acc: 0.8526 - val_loss: 0.6427 - val_acc: 0.7206\n",
      "\n",
      "Epoch 00495: val_loss did not improve from 0.52977\n",
      "Epoch 496/500\n",
      "1125/1125 [==============================] - 0s 84us/step - loss: 0.3279 - acc: 0.8546 - val_loss: 0.6352 - val_acc: 0.7233\n",
      "\n",
      "Epoch 00496: val_loss did not improve from 0.52977\n",
      "Epoch 497/500\n",
      "1125/1125 [==============================] - 0s 91us/step - loss: 0.3330 - acc: 0.8510 - val_loss: 0.6612 - val_acc: 0.7238\n",
      "\n",
      "Epoch 00497: val_loss did not improve from 0.52977\n",
      "Epoch 498/500\n",
      "1125/1125 [==============================] - 0s 82us/step - loss: 0.3293 - acc: 0.8521 - val_loss: 0.6789 - val_acc: 0.7177\n",
      "\n",
      "Epoch 00498: val_loss did not improve from 0.52977\n",
      "Epoch 499/500\n",
      "1125/1125 [==============================] - 0s 72us/step - loss: 0.3278 - acc: 0.8548 - val_loss: 0.6531 - val_acc: 0.7209\n",
      "\n",
      "Epoch 00499: val_loss did not improve from 0.52977\n",
      "Epoch 500/500\n",
      "1125/1125 [==============================] - 0s 111us/step - loss: 0.3302 - acc: 0.8517 - val_loss: 0.6480 - val_acc: 0.7206\n",
      "\n",
      "Epoch 00500: val_loss did not improve from 0.52977\n"
     ]
    }
   ],
   "source": [
    "hist = sider_model.fit(X_train, y_train, \n",
    "                       validation_data=(X_valid,y_valid),epochs=EP, batch_size=BATCH_SIZE, \n",
    "                       callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fbb3c6e64a8>]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.ylim(0., 1.)\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.plot(hist.epoch, hist.history[\"loss\"], label=\"Train loss\")\n",
    "plt.plot(hist.epoch, hist.history[\"val_loss\"], label=\"Valid loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "sider_model.load_weights('sider_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = sider_model.predict(X_valid)\n",
    "pred_nn_t = np.copy(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Find_Optimal_threshold' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-9a315c2382e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mthreshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFind_Optimal_threshold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#print(threshold)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Find_Optimal_threshold' is not defined"
     ]
    }
   ],
   "source": [
    "threshold = Find_Optimal_threshold(y_valid, pred)\n",
    "#print(threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
